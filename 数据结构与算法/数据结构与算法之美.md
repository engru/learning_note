# 数据结构与算法之美

# 一、时间复杂度

### 1.概述

​	**代码执行时间 T(n)=O(f(n))**

其中 T(n) 表示代码执行的时间，n 表示数据规模的大小，f(n) 表示每行代码执行的次数总和。

其中 大 O 时间复杂度实际上并不具体表示代码执行的时间，而是表示代码执行时间随数据规模增长的变化趋势，，所以，也叫渐近时间复杂度，简称时间复杂度。

1. **只关注执行循环最多的一段代码，总复杂度等于量级最大那段代码的复杂度**

2. 嵌套代码的复杂度等于嵌套内外代码复杂度的乘积。

​	时间复杂度表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。

### 2.几种常见的复杂度：

常量阶 O(1)、对数阶 O(logn)、线性阶 O(n)、线性对数阶 O(nlogn)、平方阶 O(n^2)、立方阶 O(n^3)、K 次方阶 O(n^k)、指数阶 O(2^n)、阶乘阶 O(n!)

对这些复杂度量级，可以分为多项式量级和非多项式量级。其中非多项式量级只有指数阶和阶乘阶，这两个量级的算法问题又叫做 NP 难问题。

#### 1.O(1)

​	只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)。或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。

#### 2. O(logn)、O(nlogn)

​	当已处理的数据规模呈等比数列级别时，代码的复杂度就是对数级别 O(logn)。而 O(nlogn) 和 O(logn) 联系其实很紧密，将 O(logn) 级别的代码执行 O(n) 级别的次数，就是 O(nlogn) 了。归并排序、快速排序的时间
复杂度都是 O(nlogn)。

#### 3.对 O(m+n)、 O(m*n) 的理解

​	之所以有两个参数，是因为代码的复杂度由两个数据的规模决定。因为无法实现知道 m 和 n 两个数据规模谁更大，所以只能都写出来。

![image-20190215170051760](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190215170051760.png)

### 3.复杂情况下的时间复杂度

​	为了表示代码在不同情况下的不同时间复杂度，我们需要引入三个概念**:最好情况时间复杂度、**
**最坏情况时间复杂度和平均情况时间复杂度。**

1.最坏情况时间复杂度:代码在最理想情况下执行的时间复杂度。
2.最好情况时间复杂度:代码在最坏情况下执行的时间复杂度。
3.平均时间复杂度:用代码在所有情况下执行的次数的加权平均值表示。
4.均摊时间复杂度:在代码执行的所有复杂度情况中绝大部分是低级别的复杂度，个别情况是
高级别复杂度且发生具有时序关系时，可以将个别高级别复杂度均摊到低级别复杂度上。基
本上均摊结果就等于低级别复杂度。

# 二、数组

## 概念

1. 数据是一种**线性表**数据结构，所谓线性表，就是数据排成像一条线一样的数据结构。
2. 这里数组用一组**连续的内存空间**，来存储**相同类型的数据**。

![image-20190215234850951](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190215234850951.png)

![image-20190215234905780](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190215234905780.png)

数组支持**随机访问**。这个特点也是因为它占有连续的内存空间。

我们拿一个长度为 10 的 int 类型的数组 int[] a = new int[10] 来举例。在我画的这个图中，计算机给数组 a[10]，分配了一块连续内存空间 1000~1039，其中，内存块的首地址为base_address = 1000。

![image-20190215235021439](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190215235021439.png)

​	计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址:

```
a[i]_address = base_address + i * data_type_size
```

​	其中 data_type_size 表示数组中每个元素的大小。我们举的这个例子里，数组中存储的是 int类型数据，所以 data_type_size 就为 4 个字节。

**二维数组内存寻址:**
对于 m * n 的数组，a [ i ][ j ] (i < m,j < n)的地址为:

```
address = base_address + ( i * n + j) * type_size
```

当面试时，我们不应该说数组的查找时间复杂度是 O(1)，排序好的数组，用二分查找，时间复杂度是 O(logn)。正确的表述是，根据下标随机访问的时间复杂度是 O(1)。

但是也是因为这个原因，数组的插入和删除非常“低效”。为了保持连续性，需要做大量的数据迁移工作。

## 插入

如果数据是有序的，每次插入到数组的第 k 个位置，需要把 k~n 这部分数据都往后移以为，若是在每个位置插入元素的概率是一样的，那么平均时间复杂度是 (1+2+...n)/n=O(n)。

若数据是无序的，数组只是一个存储数据的集合，这种情况下，要把数据插入到第 k 个位置，可以尝试把第 k 个元素移到数组的最后面，把新元素插入到第 k 个位置，这样在特定场景下，插入一个元素到第 k 个位置时间复杂度可以降为 O(1)。

## 删除

和插入一样，最好情况下时间复杂度是 O(1)，如果删除开头的数据，则是最坏情况时间复杂度 O(n)，平均情况下时间复杂度是 O(n)。

如果我们将多次删除操作集中在一起删除，就可以提高删除的效率，这也是 jvm 的标记清楚垃圾回收算法。

## 容器

ArrayList 相比数组，最大的优势就是将许多细节封装起来了，比如前面提到的数组插入、删除时需要搬移其他数据等。另外的优势就是自动扩容了。

但不是所有情况都需要用到 ArrayList。比如

1. ArrayList 无法存储基本类型。自动封箱拆箱需要性能消耗。
2. 有些操作较为简单，无需用到 ArrayList。
3. 定义多维数组时，若是用 ArrayList 看起来不直观。

![image-20190215235559326](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190215235559326.png)

# 三、链表

​	缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常 见的 **CPU 缓存、数据库缓存、浏览器缓存等等。** 

​	缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留?这就需要 缓存淘汰策略来决定。**==常见的策略有三种:先进先出策略 FIFO(First In，First Out)、最少使 用策略 LFU(Least Frequently Used)、最近最少使用策略 LRU(Least Recently Used)。==** 

## 数组和链表的内存比较

![image-20190217105102418](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217105102418.png)

​	从图中我们看到，数组需要一块连续的内存空间来存储，对内 存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储 空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。 

而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起 来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。 

​	链表是通过“指针”将一组零散的内存块串联起来的数据结构，相比数组就是使用一组连续的内存块来存储数据的数据结构。

常见的链表有：

-  **单链表**： 每个数据块只有一个指针指向下一节点的数据。其中有两个特殊的节点，一个是头节点，数据块是空的，只有一个指针指向下一节点；另一个节点是尾节点，他的特点是，指针是指向空地址 NULL，表示这是链表的最后一个节点。![image-20190217105145325](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217105145325.png)
-  **循环链表**： 循环链表是，每个数据块都有一个指针指向下一节点，尾节点的指针指向头节点，另有一个单独的头指针指向开头。![image-20190217105345482](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217105345482.png)
-  **双链表**： 双链表是在单链表的基础上，**在每个数据块节点上增加一个指针指向上一节点。**每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针prev 指向前面的结点。![image-20190217105948132](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217105948132.png)
-  **双向循环链表**：双向循环链表是每个数据块都有两个指针，一个向前指向，一个向后指向，另有一个单独的指针指向一个数据块节点，这个指针是头指针。![image-20190217110211704](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217110211704.png)

## 性能

​	==链表的随机访问性能是 O(n),数组是 O(1)。链表的插入、删除操作的性能是 O(1)，数组是 O(n)。==

​	这里的链表，插入，删除，是指知道要插入的点，删除的点的指针，比如单链表插入时知道插入点上一节点的指针时，通过改变指针的指向就可以完成 O(1) 时间内的数据插入，删除同理。

​	若是单链表删除时，不知道指向要删除的具体节点的指针，那么就要在删除前先进行随机访问，那么性能就是 O(n)。

​	对于有序双向链表，查询效率会比单链表高一点，因为我们可以记录上次查找的位置 P，每次查询时，根据要查询的值与 P 的大小关系，决定往前还是往后查找，所以平均只需要查找一般的数据。

**在 Java 语言中，LinkedHashMap 这个容器，他的实现原理就用到了双向链表这个数据结构。**

## 数组 VS 链表

​	数组简单易用，在实现上是使用连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中不是连续存储，对 CPU 缓存不友好，没办法有效预读。

​	**数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足(out of memory)”。**如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。	

​	但数组的特点也是它的不足，他的内存空间是固定的，如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致内存不足(out of memory)，例如如果现在系统由 100M 不连续的内存空间，声明 100M 数组就会失败，另外到数组扩容时，复制原数组的内容到新数组也很费时。这就是数组和链表最大的区别。

另外如果我们的代码对内存使用非常苛刻，那应该使用数组，比如安卓之类的，因为链表需要维护额外的空间去存储指针。而且对链表进行频繁的插入、删除操作，还对导致频繁的内存申请和释放，容易造成内存碎片。如果是 Java 语言，就可能导致频繁的 gc。![image-20190217110226500](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217110226500.png)

## 如何书写正确的链表代码

### 技巧一：理解指针或引用的含义

​	**将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向这个变量，通过指针就能找到这个变量。**

如：在编写链表代码的时候，我们经常会有这样的代码:p->next=q。这行代码是说，p 结点中的 next 指针存储了 q 结点的内存地址。 

还有一个更复杂的，也是我们写链表代码经常会用到的:p->next=p->next->next。这行代码 表示，p 结点的 next 指针存储了 p 结点的下下一个结点的内存地址。 

### 技巧二：警惕指针丢失和内存泄漏

​	写链表时，一定要注意指针指向哪了，对于脑子转不过来的情况，可以在纸上画图辅助思考。
 对于自己管理内存的语言，如 C 语言，如果没有手动释放节点对应的内存空间，就会产生内存泄漏。不过，对于 Java 这种虚拟机自动管理内存的编程语言来说，就不需要考虑那么多了。

### 技巧三：利用哨兵简化实现难度

​	有时代码写不出来，也是因为代码的小逻辑多而乱，如果能够实现分析代码，简化逻辑，那么写起代码来就会更容易轻松了。
 比如，**当在单链表插入一个新节点时，需要两个小逻辑：1. *链表是空的的情况* 2. *链表不是空的情况* 写起来的代码就会像这样复杂。而如果我们有一个哨兵节点，那么就只需“无脑”往这个节点后面插入新节点而不用进行一次判空特殊处理了。**

其实就是加了一个虚拟头节点，这个结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了。

**拓展**：==在很多算法中都有用到哨兵做简化，比如插入排序、归并排序、动态规划等==。
 下面这个例子就是用了哨兵提升性能：

```Java
inf find(char* a, int n, int key) {
  if (a[n-1] == key) {
    return n-1;
  }
  char tmp = a[n-1];
  a[n-1] = key;
  int i = 0;
  while (a[i] != key) {
    ++i;
  }
  a[n-1] = tmp;
  if (i == n-1) return -1;
  return i;
}
```

### 技巧四：重点留意边界条件处理

软件开发中，我们往往是从“通常情况入手，设计代码”，这种情况下，如果不注意特殊情况（边界情况）时，就容易产生 BUG。一定要在写完代码后，检查边界条件是否考虑齐全。
 常用来检查链表代码是否正确的边界条件有这样几个：

- 如果链表为空时，代码能否正常工作？
- 如果链表只包含一个节点时，代码能否正常工作？
- 如果链表只包含两个节点时，代码能否正常工作？
- 代码逻辑在处理头节点和尾节点时，能否正常工作？

### 技巧五：画图举例，辅助思考

感觉这也是软件设计图的细节版。写出来，整理一下，就明白了。![image-20190217120701097](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217120701097.png)

### 技巧六：多写多练，没有捷径

#### 5个常见的链表操作：

- 单链表反转
- 链表中环的检测
- 两个有序的链表合并
- 删除链表倒数第 n 个结点
- 求链表的中间结点

对应LeetCode题目：206，141，21，19，876

# 四、栈

栈，是一种“操作受限”的线性表，只允许在一端插入和删除数据。后进后出是它的最大特点。

## 栈的实现

​	栈可以用数组和链表实现。数组实现的栈叫顺序栈，链表实现的栈叫链式栈。特别要注意的是，顺序栈的动态扩容，应用平摊分析法，最终分析出插入的时间复杂度仍是 O(1)。![image-20190217131404224](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217131404224.png)

## 栈的应用

1. **函数调用栈**
    操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构，用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈。**当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。![image-20190217150150940](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217150150940.png)**![image-20190217150202171](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217150202171.png)
2. **栈在函数表达式中的应用**
    编译器就是通过两个栈来实现表达式的运算的，一个保存操作数的栈，另一个保存运算符的栈。我们从左向右遍历表达式，**当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶符号比较。如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，**从操作数的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。
   1. 例子：3+5*8-6![image-20190217151249246](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217151249246.png)
3. **栈在括号匹配中的应用**
    **我们用栈来保存未匹配的左括号**，从左到右依次扫描字符串。**当扫描到左括号时，则讲其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。**
    当所有括号都扫描完成之后，如果栈为空，则说明字符串是合法格式；否则，说明有未匹配的左括号，为非法格式。

## 课后思考

1. **我们在讲栈的应用时，讲到用函数调用栈来保存临时变量，为什么函数调用要用“栈”来保存临时变量呢？其他数据结构不行吗？**

特定数据结构是特定应用场景的抽象。

函数调用的顺序，符合先进者后出，后进者先出的特点，还有函数中调用函数，也是这样，先开始执行的函数，必须等到内部调用的其他函数执行完毕，该函数才能执行结束。

比如函数中的局部变量的生命周期的长短，是先定义的局部变量生命周期长，后定义的局部变量生命周期短。

1. **我们都知道，JVM 内存管理中有个“堆栈”的概念。栈内存用来存储局部变量和方法调用，堆内存用来存储 Java 中的对象。那么 JVM 里面的“栈”跟我们这里说的“栈”是不是一回事呢？如果不是，那它又为什么叫做“栈”呢？**

内存中的堆栈和数据结构堆栈不是一个概念。内存中的堆栈是真实存在的物理区，数据结构中的堆栈是抽象的数据结构。
 内存空间在逻辑上分为三部分：代码区、静态数据区和动态数据区。动态数据区又分为栈区和堆区。
 代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换。
 静态数据区：存储全局变量、静态变量、常量，常量包括 final 修饰的常量和 String 常量。系统自动分配和回收。
 栈区：存储方法的形参、局部变量、返回值。由系统自动分配和回收。
 堆区：new 操作符创建的一个对象的引用地址存储在栈区，指向该对象存储在堆区中的真实数据。

# 五、队列

队列也是一种“操作受限”的线性表，只支持两种基本操作：入队和出队。

队列的应用非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层的系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等。

​	**队列有两个指针，一个head指针和一个tail指针，分别指向队头和队尾，当插入元素的时候，即入队的时候，tail指针向右移动，head指针不变；当出队的时候，head指针向右移动，tail指针不变**

![image-20190224230644568](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190224230644568.png)

​	当我们调用两次出队操作之后，队列中 head 指针指向下标为 2 的位置，tail 指针仍然指向下标为 4 的位置。![image-20190224230659386](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190224230659386.png)

​	当队列的 tail 指针移动到数组的最右边后，如果有新的数据入队，我们可以将 head 到 tail 之间的数据，整体搬移到数组中 0 到 tail-head 的位置。![image-20190224230807069](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190224230807069.png)

## 顺序队列和链式队列

用数组实现的队列叫顺序队列，用链表实现的队列叫链式队列。

==顺序队列在没有空闲空间时，需要触发一次数据的搬移操作==。

## 循环队列

![image-20190224231004881](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190224231004881.png)

​	图中这个队列的大小为 8，当前 head=4，tail=7。当有一个新的元素 a 入队时，我们放入下标为 7 的位置。但这个时候，我们并不把 tail 更新为 8，而是将其在环中后移一位，到下标为 0 的位置。当再有一个元素 b 入队时，我们将 b 放入下标为 0 的位置，然后tail 加 1 更新为 1。所以，在 a，b 依次入队之后，循环队列中的元素就变成了下面的样子:![image-20190224231118349](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190224231118349.png)

​	将顺序队列首尾连接起来就是循环队列。要注意的点就是，队列判满的条件是==(tail + 1) % n = head==。如果 head == tail 表示队列为空。

队满的情况：

![image-20190224231304271](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190224231304271.png)

​	当队列满时，图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。

```java
public class CircularQueue {
    // 数组:items，数组大小:n
    private String[] items;
    private int n = 0;
    // head 表示队头下标，tail 表示队尾下标
    private int head = 0;
    private int tail = 0;

    // 申请一个大小为 capacity 的数组
    public CircularQueue(int capacity) {
        items = new String[capacity];
        n = capacity;
    }

    //入队
    public boolean enqueue(String item) {
        // 队列满了
        if ((tail + 1) % n == head) return false;
        items[tail] = item;
        tail = (tail + 1) % n;
        return true;
    }

    //出队
    public String dequeue() {
// 如果 head == tail 表示队列为空
        if (head == tail) return null;
        String ret = items[head];
        head = (head + 1) % n;
        return ret;
    }
}
```

## 阻塞队列和并发队列

​	**阻塞队列**就是在队列的基础上增加了阻塞操作。简单来说，就是在队列为空的时候，出队操作会被阻塞，当队列满的时候，入队操作会被阻塞。其实，上面的定义，就是一个“**生产者-消费者**”模型。

​	这种模型，可以有效协调生产和消费的速度。当“生产者”生产速度过快，“消费者”来不及消费时，存储队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续生产。

​	而且不仅如此，基于阻塞队列，我们还可以协调“生产者”和“消费者”的个数，来提高数据处理的效率，比如，我们可以多配置几个“消费者”，来对应一个“生产者”。

**并发队列**：线程安全的队列我们叫做并发队列。最简单的实现方式就是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低。实际上，基于 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。

## 队列在线程池中的应用

当线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？

我们一般有两种处理策略一种是非阻塞队列，一种是阻塞队列。

阻塞队列又有两种实现方式，一种是无限排队等待，另一种就是基于数组的有限队列，这里，给数组队列设置一个合理的队列大小，是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。

## 基于链表和数组的队列的区别

基于链表的实现方式，可以实现一个支持**无限排队的无界队列(unbounded queue)**，但是可 能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系 统，基于链表实现的无限排队的线程池是不合适的。 

而基于数组实现的有界队列(bounded queue)，队列的大小有限，所以线程池中排队的请求 超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。

==可以使用 cas + 数组的方式实现无锁并发队列。分布式消息队列，如 kafka 也是一种队列。==

# 六、递归

## 如何理解递归

个人觉得递归就是“递” + “归” + 栈。生成子问题是递的过程，这个过程是把函数和变量压入栈，从子问题返回母问题的过程是归，这个过程把函数和变量从栈中取出。

## 递归的三个条件

1. 一个为题的解可以分解为几个子问题的解。
2. 这个问题和分解之后的子问题，除了数据规模不同，求解思路完全一样。
3. 存在递归终止条件。

## 如何编写递归代码

**写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。**

另外，编写递归代码还有一个关键，那就是遇到递归，就把它抽象成一个递推公式，不用想一层一层的调用关系，不要试图用人脑去分解递归的每个步骤。

## 递归要注意的点

- **递归代码要警惕堆栈溢出**

函数调用使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，就会有堆栈溢出的风险。

我们可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归超过一定深度后就不继续往下递归了。

- **递归代码要警惕重复计算**

如果递归分解出来的子问题很多都是重复计算的话，可以考虑通过一个数据结构（比如散列表）来保存求解过的结果。当递归调用到求解过的结果时，直接返回求解过的结果即可。

- **时间和空间成本**

在时间效率上，递归代码里有很多函数调用，这些函数调用的数量较大时，就会积聚成一个可观的成本。

在空间效率上，递归调用一次就会在内存栈中保存一次现场数据，这部分也会占用一定的空间。

## 怎么改递归代码为非递归代码

因为递归本身就是借助栈来实现的，只不过我们使用的栈是系统或者虚拟机本身提供的，我们没有感知罢了。如果我们在自己的内存堆上实现栈，手动模拟入栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样子。不过这样仍然不能避免递归的某些弊端，而且徒增了递归的复杂度。

另一种正常一点的改写成非递归代码的方法是，根据初始条件和递归关系，手动用循环模拟每一层的操作。

## 课后思考

我们平时调试递归代码喜欢使用 IDE 的单步跟踪功能，像规模较大、递归层次很深的递归代码，几乎无法使用这种调试方式。对于递归代码，你有什么好的调试方式呢？

1. 打印出关键日志
2. 设置条件断点，进入特定层次位置。

另外，调试递归代码的方式，同样适合调试循环轮次较多的情况。

# 七、排序

![image-20190225153746922](/Users/jack/Desktop/md/images/image-20190225153746922.png)

## 1.如何分析排序算法

### 算法的执行效率

1. **最好情况、最坏情况、平均时间复杂度**

要了解这三种情况分别对应什么样的原始数据输入。

 2.**时间复杂度的系数、常数、低阶**

​	时间复杂度是当数据规模 n 很大时的一个增长趋势，所以可以忽略系数、常数、低阶。
 但在实际软件开发中，我们可能要排序的是 10 个、100 个、 1000 个这样的小规模数据。这时我们就要分析每个整个 T(n），而不仅仅是 O(n) 了。

3.**比较次数和交换（或移动）次数**

​	排序算法分为基于比较的算法，还有不基于比较的算法。如果我们分析基于比较的算法，那就同时把这两种操作：比较和交换 一起考虑。

​	比如我们有一组数据2，9，3，4，8，3，按照大小排序之后就是2，3，3，4，8，9。 这组数据里有两个3。经过某种排序算法排序之后，如果两个3的前后顺序没有改变，那我们就把这种排序算法叫作稳定的排序算法;如果前后顺序发生变化，那对应的排序算法就叫作不稳定的排序算法。

​	比如说，**我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。**如果我们现在有10万条订单数据，我们希望按照 金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。

> ​	最先想到的方法是:我们先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间排序。这种排序思路理解起来不难，但是实现起来会很复杂。
> ​	借助稳定排序算法，这个问题可以非常简洁地解决。解决思路是这样的:我们先按照下单时间给订单排序，注意是按照下单时间，不是金额。排序完成之后，我们用稳定排序算法，按照订单金额重新排序。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。
> ​	因为稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。第一次排序之后，所有的订单按照下单时间从早到晚有序了。在第二次排序中，我们用的是稳定的排序算法，所以经过第二次排序之后，相同金额的订单仍然保持下单时间从早到晚有序。

### 算法的内存消耗

算法的内存消耗应该被考虑。不过还可以了解一个概念：**原地排序特指空间复杂度是 O(1) 的排序算法。**

### 排序算法的稳定性

排序算法的稳定性指的是，如果待排序列中存储了相同的元素，那么经过排序之后，相等的元素之间原有的先后顺序不变。

## 2.O(n2) 的排序算法

### 冒泡排序

​	**冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻两个元素进行比较，看是否满足大小关系要求，如果不满足就让它俩互换。**一次冒泡排序会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再进行后续的冒泡操作。

```Java
// 冒泡排序，a 表示数组，n 表示数组大小
public void bubbleSort(int[] a, int n) {
  if (n <= 1) return;
 
 for (int i = 0; i < n; ++i) {
    // 提前退出冒泡循环的标志位
    boolean flag = false;
    for (int j = 0; j < n - i - 1; ++j) {
      if (a[j] > a[j+1]) { // 交换
        int tmp = a[j];
        a[j] = a[j+1];
        a[j+1] = tmp;
        flag = true;  // 表示有数据交换      
      }
    }
    if (!flag) break;  // 没有数据交换，提前退出
  }
}
```

### 插入排序

插入排序的过程，就是不断将一个新的元素插入到一个有序的数组的过程。

```java
// 插入排序，a 表示数组，n 表示数组大小
 /**
 * 首先对数组的前两个数据进行从小到大的排序。
 * 接着将第三个数据与排好序的两个数据比较，将第三个数据插入合适的位置。
 * 然后将第四个数据插入到已排好序的前3个数据中。
 * 其实就是每次都是一个区域内的排序比较大小
 **/
public void insertionSort(int[] a, int n) {
  if (n <= 1) return;

  for (int i = 1; i < n; ++i) {
    int value = a[i];
    int j = i - 1;
    // 查找插入的位置
    for (; j >= 0; --j) {
      if (a[j] > value) {
        a[j+1] = a[j];  // 数据移动
      } else {
        break;
      }
    }
    a[j+1] = value; // 插入数据
  }
}
```

### 选择排序

​	选择排序和插入排序类似，也是区分已排序区间和未排序区间。但是选择排序每次是选择未排序区间中最小的元素，将其添加到已排序列的末尾。而选择排序是固定选择第一个未排序的元素插入到已排序序列的合适位置。

```java
  public static void selectionSort(int[] a, int n) {
    if (n <= 1) return;
    for (int i = 0; i < n - 1; ++i) {
      // 查找最小值
      int minIndex = i;
      for (int j = i + 1; j < n; ++j) {
        if (a[j] < a[minIndex]) {
          minIndex = j;
        }
      }

      // 交换
      int tmp = a[i];
      a[i] = a[minIndex];
      a[minIndex] = tmp;
    }
  }
```

![image-20190407105104276](/Users/jack/Desktop/md/images/image-20190407105104276.png)

### 2.1 三个问题

#### 是原地排序算法吗？

​	这三个排序都是在原数组上进行排序排序，只需要常量级的临时空间，所以它们的空间复杂度是 O(1)，是原地排序算法。

#### 是稳定的排序算法吗？

- 在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。

冒泡排序和插入排序是稳定的排序算法，选择排序不是稳定的排序算法。

冒泡排序不对相同的元素做交换。插入排序把值相同的元素，将后面出现的元素插入到前面的元素后面，所以也是稳定的算法。

选择排序会找出未排序元素中的最小值，和未排序元素的第一个做交换，这个交换是不稳定的。

### 2.2 时间复杂度是多少？

#### 1.有序度，逆序度，满有序度

​	有序度是数组中具有有序关系的元素对的个数。完全有序的数组，有序度就是 n*(n-1)/2，它的有序度叫满有序度。逆序度跟有序度相反。

==总结公式就是 逆序度 = 满有序度 - 有序度==

#### 2.冒泡排序

​	冒泡排序最好情况下时间复杂度是 O(n)，即要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了。

最坏情况的时间复杂度是 O(n2)，这时要排序的数组是逆序的，需要进行 n 次冒泡操作。

对于冒泡排序的时间复杂度分析如下：冒泡排序每次交换，减少一个逆序度。故冒泡排序的具体每次执行的时间跟输入数据的逆序度息息相关，假如输入一个完全有序的数组，那么这个数组的逆序度就是 0，只需要进行 0 次交换，假如输入的数组是一个完全逆序的数组，那么它的逆序度就是 n*(n-1)/2，需要进行 n*(n-1)/2 次交换。我们不严谨地取个中间值 n*(n-1)/4，表示初始有序度不是很高也不是很低的平均情况。按照这样，平均时间复杂度就是 O(n2)。

1. 插入排序

插入排序最好的时间复杂度是 O(n)，即要排序的数组已经有序，我们从尾到头每次只需要比较一个数据就可以确定插入的位置。

插入排序最坏的时间复杂度是 O(n2)，即待排序的数组完全逆序，每次插入都相当于在数组的第一个位置插入新的数组，这个插入的时间复杂度我们在数组那节分析过，是 O(n)，又一共有 n 个元素，所以最终的复杂度就是 O(n2)。

同样，我们在数组那节分析过，在数组插入一个数据的平均时间复杂度是 O(n)，所以，对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以平均时间复杂度是 O(n2)。

1. 选择排序

选择排序最好、最坏、平均时间复杂度都是 O(n2)。

选择排序每次需要比较整个未排序数组的的所有元素，找出最小的元素插入到已排序数组的最末端。这样每轮的时间复杂度是 O(n)，循环执行 n 次，时间复杂度就是 O(n2)。不受数组排列顺序的影响。

### 2.3 相关问题

**冒泡排序和插入排序的时间复杂度都是 O(n2)，都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎？**

> ​	我们在前面分析过，冒泡排序不管怎么优化，元素交换的次数是一个固定的值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，元素移动的次数等于原始数据的逆序度。
>
> ​	但是，仔细分析下，冒泡排序的交换操作要比插入排序的移动操作复杂得多，==冒泡排序每次需要做三个赋值操作，而插入排序只需要做一个移动操作。==

```java
冒泡排序中数据的交换操作：
if (a[j] > a[j+1]) { // 交换
   int tmp = a[j];
   a[j] = a[j+1];
   a[j+1] = tmp;
   flag = true;
}

插入排序中数据的移动操作：
if (a[j] > value) {
  a[j+1] = a[j];  // 数据移动
} else {
  break;
}
```

​	如果不严谨地把每次操作地时间粗略估计为单位时间 (unit_time)，然后分别用冒泡排序和插入排序对同一个逆序度为 K 的数组进行排序。用冒泡排序，需要进行 K 次交换操作，每次需要 3 个赋值语句，所以交换操作总耗时 3 * K 个单位时间。而插入排序中移动数据的总时间为 K 个单位时间。

**特定的算法是依赖特定的数据结构的。上面几种算法都是基于数组的。如果将数据结构存储在链表中，这三种排序还能正常工作吗？如果能，那相应的时间、空间复杂度是多少？**

> ​	对于这个问题，首先要明确一个前提，是否允许修改链表节点的 value 值，还是只能改变节点的位置。一般而言，考虑只能改变节点位置的情况。冒泡排序相比于数组实现，比较次数一致，但是交换操作更加复杂了。插入排序相比于数组实现，比较次数一致，但是插入操作不需要再改变后续节点，但如果是单链表，排序结束后可能要倒置链表。而对于选择排序，比较次数和插入次数都一致，但是由于链表的插入操作更复杂一点，可能会比数组操作慢一点点，变化不大。
>
> 综上，冒泡排序会慢一些，而选择排序系数会减小，选择排序变化不大。

## 3.归并排序

​	归并排序和快排都用到了分冶思想，可以借助这个思想解决非排序问题。比如：如何在**O(n)**的时间复杂度内查找一个无序数组中的第**K**大元素? 

### 3.1 原理(先分后合)

​	归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。

​	分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。

> 递推公式: merge_sort(p...r) = merge(merge_sort(p...q), merge_sort(q+1...r)) 
>
> 终止条件:p >= r 不用再继续分解 
>
> merge_sort(p...r)表示，给下标从p到r之间的数组排序。我们将这个排序问题转化为了两个子问题，merge_sort(p...q)和merge_sort(q+1...r)，其中下标q等于p和r的中间位置，也就是(p+r)/2。当下标从p到q和从q+1到r这两个子数组都排好序之后，我们再将两个有序的子数组合并在一起，这样下标从p到r之间的数据就也排好序了。 
>
> 即：取数组中间位置，将数组拆分为两个小数组，然后分别进行排序，最后再进行合并。

伪代码：

```Java
// 归并排序算法, A是数组，n表示数组大小 merge_sort(A, n) {
}merge_sort_c(A, 0, n-1)
// 递归调用函数 merge_sort_c(A, p, r) {
// 递归终止条件
if p >= r then return
// 取p到r之间的中间位置q q = (p+r) / 2
// 分治递归 merge_sort_c(A, p, q) merge_sort_c(A, q+1, r)
// 将A[p...q]和A[q+1...r]合并为A[p...r] }merge
```

​	**merge(A[p...r], A[p...q], A[q+1...r])这个函数的作用就是，将已经有序的A[p...q]和A[q+1...r]合并成一个有序的数组，并且放入A[p...r]。**

如图所示，我们申请一个临时数组tmp，大小与A[p...r]相同。我们用两个游标i和j，分别指向A[p...q]和A[q+1...r]的第一个元素。比较这两个元素A[i]和A[j]，如 果A[i]<=A[j]，我们就把A[i]放入到临时数组tmp，并且i后移一位，否则将A[j]放入到数组tmp，j后移一位。 

继续上述比较过程，直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的 就是两个子数组合并之后的结果了。最后再把临时数组tmp中的数据拷贝到原数组A[p...r]中。 

![image-20190415082224553](/Users/jack/Desktop/md/images/image-20190415082224553.png)

### 3.2 性能分析

归并排序是一个稳定的排序算法。

#### 时间复杂度：

​	因为合并排序是通过递归去解决的，假设一个问题a可以分解为多个子问题b、c，那求解问题a就可以分解为求解问题b、c，求解完b，c两个问题之后才将它们的结果合并。所以我们定义求解问题a的时间是T(a)，求解问题b、c的时间分别是T(b)和 T( c)，那我们就可以得到这样的递推关系式:

> T(a) = T(b) + T(c) + K
>
> 其中K等于将两个子问题b、c的结果合并成问题a的结果所消耗的时间。

​	我们假设对n个元素进行归并排序需要的时间是T(n)，那分解成两个子数组排序的时间都是T(n/2)。我们知道，**merge()函数合并两个有序子数组的时间复杂度是O(n)。**所以，套用前面的公式，归并排序的时间复杂度的计算公式就是: T(1) = C; n=1时，只需要常量级的执行时间，所以表示为C。 

> T(n) = 2*T(n/2) + n; n>1 

通过这个公式，如何来求解T(n)呢?

> T(n) = 2*T(n/2) + n
>  = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n
>  = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n
>  = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n .
>
> .....
>  = 2^k * T(n/2^k) + k * n 

​	通过这样一步一步分解推导，我们可以得到T(n) = 2^k*T(n/2^k)+k*n。当T(n/2^k)=T(1)时，也就是n/2^k=1，我们得到k=log2n 。我们将k值代入上面的公式，得到T(n)=C*n+n*log2n 。如果我们用大O标记法来表示的话，T(n)就等于O(nlogn)。所以==归并排序的时间复杂度是O(nlogn)。== 

​	**从我们的原理分析和伪代码可以看出，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是O(nlogn)。** 

#### 空间复杂度：

​	**归并排序不是原地排序算法。**这是因为归并排序的合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间。

> ​	实际上，递归代码的空间复杂度并不能像时间复杂度那样累加。刚刚我们忘记了最重要的一点，那就是，**尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。**==在任意时刻，CPU只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过n个数据的大小，所以空间复杂度是O(n)。==

```Java
public static void mergeSort(int[] arr) {
        if (arr == null || arr.length < 2) {
            return;
        }
        mergeSort(arr, 0, arr.length - 1);
    }

    /**
     * @param arr   待排序的数组
     * @param l     数组的左边界
     * @param r     数组的右边界
     * 归并排序，递归排序后调用merge方法将两个数组合并
     */
    public static void mergeSort(int[] arr, int l, int r) {
        if (l == r) {
            return;
        }
        // 避免数值溢出，所以用这种方式计算中间的数，>>表示除以2的1次方
        int mid = l + ((r - l) >> 1);
        mergeSort(arr, l, mid);
        mergeSort(arr, mid + 1, r);
        merge(arr,l,mid,r);
    }

    /**
     * @param arr       待merge的数组
     * @param l         数组的左边界
     * @param m         数组的中间数
     * @param r         数组的右边界
     */
    public static void merge(int[] arr, int l, int m, int r) {
//        创建一个跟arr一样长度的数组，作为存储arr的值,help是一个辅助数组
        int[] help = new int[r - l + 1];
        int i = 0;
//        p1,p2分别是两个分组中最小的值，即分别指向排序好后的两个数组的第一位
        int p1 = l;
        int p2 = m + 1;
//        对help进行赋值和排序，分为前后两部分赋值,哪边小填哪个，就一个排序的过程
        while (p1 <= m && p2 <= r) {
            // 每次存入之后，对应的数组索引就加一
            help[i++] = arr[p1] < arr[p2] ? arr[p1++] : arr[p2++];
        }
//        如果上面的循环跳出之后，那么p1,p2必有一个越界，需要存入还没越界的值
// 下面两个while循环不会同时运行，如果p1越界，则p2不越界，存入p2的值
        while (p1 <= m) {
            help[i++] = arr[p1++];
        }
        while (p2 <= r) {
            help[i++] = arr[p2++];
        }
        for (i = 0; i < help.length; i++) {
            arr[l + i] = help[i];
        }
    }
```

## 4.快速排序

### 4.1 原理分析

​	**快排的思想是这样的:如果要排序数组中下标从p到r之间的一组数据，我们选择p到r之间的任意一个数据作为pivot(分区点)。**

> ​	我们遍历p到r之间的数据，将小于pivot的放到左边，将大于pivot的放到右边，将pivot放到中间。经过这一步骤之后，数组p到r之间的数据就被分成了三个部分，前面p到q-1之间都是小于pivot的，中间是pivot，后面的q+1到r之间是大于pivot的。

![image-20190415084954962](/Users/jack/Desktop/md/images/image-20190415084954962.png)

主要在于partition()分区函数，采取原地分区的做法：

> partition(A, p, r) {
>
> ​	 pivot := A[r]
> ​	 i := p
> ​	 for j := p to r-1 do { 
>
> ​		if A[j] < pivot { 
>
> ​			swap A[i] with A[j] 
>
> ​	}
>
> }
>
> i := i+1 
>
> swap A[i] with A[r] return i 
>
> ​	我们通过游标i把A[p...r-1]分成两部分。A[p...i-1]的元素都是小于pivot的，我们暂且叫它“已处理区间”，A[i...r-1]是“未处理区间”。我们每次都从未处理的区间A[i...r-1]中取一个元素A[j]，与pivot对比，如果小于pivot，则将其加入到已处理区间的尾部，也就是A[i]的位置。

### 4.2 性能分析

​	T(n)在大部分情况下的时间复杂度都可以做到O(nlogn)，只有在极端情况下，才会退化到O(n2)。

### 4.3 解决Kth问题

​	快排核心思想就是分治和分区，我们可以利用分区的思想，来解答Kth问题:O(n)时间复杂度内求无序数组中的第K大元素。

​	比如，4， 2， 5， 12， 3这样一组数据，第3大元素就是4。 我们选择数组区间A[0...n-1]的最后一个元素A[n-1]作为pivot，对数组A[0...n-1]原地分区，这样数组就分成了三部分，A[0...p-1]、A[p]、A[p+1...n-1]。 

​	如果p+1=K，那A[p]就是要求解的元素;如果K>p+1, 说明第K大元素出现在A[p+1...n-1]区间，我们再按照上面的思路递归地在A[p+1...n-1]这个区间内查找。同 理，如果K<p+1，那我们就在A[0...p-1]区间查找。 

![image-20190225153911990](/Users/jack/Desktop/md/images/image-20190225153911990.png)

# 八、线性排序

## 桶排序

桶排序，顾名思义，会用到“桶”。核心思想是将要排序的数据分到几个有序的桶里，再对每个桶里的数据进行单独排序。最后再把每个桶里的数据按顺序连接起来，组成的序列就是有序的了。

-  **对数据的要求**： 待排数据要能很容易划分成 n 个桶，桶之间有天然的大小顺序。并且数组在各个桶之间的分布是比较均匀的。
-  **时间空间复杂度**： 假设 n 个数据均匀地分布到 m 个桶内，每个桶就有 k = n/m 个元素。对桶内每个数据使用快排，总时间复杂度 O(m * k * logk) = O(n * log(n/m))。当 m 接近 n 时，时间复杂度为 O(n)。当数据分布很不均匀时，时间复杂度会退化到 O(nlogn)。
-  **应用场景**： 可以用于外部排序。

> 比如用几百 M 内存排序 10GB 的订单数据（假设金额都是正整数）。
>
> 我们可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是 1 元，最大是 10 万元。我们将所有订单根据金额划分到 100 个桶里，第一个桶我们存储金额在 1 到 1000 元之间的订单，第二桶存储金额在 1001 元到 2000 元之间的订单，以此类推。每一个桶对应一个文件，并且按照金额大小顺序命名（00，01，02…99）。
>
> 在理想情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的数据，我们就可以将这 100 个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小打到排序的订单数据了。
>
> 不过，订单金额很可能是分布极不均匀的，若是集中在某个区间，比如 1 到 1000 元中间比较多，那我们还可以对这个区间再进行具体的划分，比如 1 到 100 是一个区间，101 到 200 是第二个区间，以此类推。如果划分之后，某个区间的订单仍然太多，那还可以继续划分，直到所有文件都可以读入内存为止。

## 计数排序

计数排序应该是桶排序的一种比较特殊的情况。这种情况要求待排序的数据集中在一个不大的范围内，比如最大值是 K ，我们就可以把数据分成 K 个桶，每个桶里的数据都是相同的，省掉了桶内排序的时间。若有负数，还需要设置偏移量。

-  **对数据的要求**： 数据集中在一个较小的范围内。
-  **时间空间复杂度**： 时间复杂度是 O(n)。只有只有三趟遍历，第一趟计数，第二趟映射到辅助数组，第三趟拷贝回原数组。空间复杂度是 O(n)，虽然省掉了桶内排序的时间，但是空间没有减少。
-  **应用场景**： 比如对高考考生成绩排序，只有几百种情况，直接开几百个桶就可以了。

## 基数排序

是一种多关键字排序方法。从关键字的后部往前进行多轮 O(n) 的稳定性排序排序。

-  **对数据的要求**： 待排数据的键值部分可以分成几个部分。
-  **时间空间复杂度**： 总时间复杂度是键值部分个数 m 的 O(n) 倍，由于键值部分个数一般是一个很小的常量，所以，总时间仍是 O(n)。空间复杂度是 O(n)。
-  **应用场景**： 对手机号进行排序。键值部分个数是 11 个，总共进行 11 趟。其他情况若是关键字部分长度不一，可以采用填充法。

## 课后思考

根据年龄给 100 万用户排序

使用桶排序，开 120 个桶，看情况要不要进行外部排序。

对包含大写字母、小写字母、数字的一个字符串按照小写字母在前，大写字母在最后，数字在中间，不用排序算法进行排列。

这是一个荷兰国旗问题。

使用三个指针，pre, current, last 分别指向小写字母部分的结尾，数字部分的开头，大写字母部分的开头，采用交换元素的方法，遍历一遍待排字符串即可。时间复杂度 O(n)，空间复杂度 O(1)。

# 九、优化快速排序

![image-20190428110501289](/Users/jack/Desktop/md/images/image-20190428110501289.png)

​	如果对小规模数据进行排序，可以选择时间复杂度是O(n2)的算法;如果对大规模数据进行排序，时间复杂度是O(nlogn)的算法更加高效。

> Java语言采用堆排序实现排序函数，C语言使用快速排序实现排序函数。
>
> 快排最坏情况的O(n2)主要原因还是==因为分区点选的不够合理。==所以，为了提高排序算法的性能，要尽可能地让每次分区都比较平均。

- 三数取中法，九数取中法
- 随机法

​	快速排序是用递归来实现的，递归要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，有两种解决办法:

- 限制递归深度
- 自己实现函数调用栈，手动模拟入栈出栈

## Glibc 中的 qsort()

- 小规模排序使用归并排序，空间换时间。
-  数据规模大时，使用快排。
-  快排采用三数取中法。
-  并采用自己实现堆上的栈，手动模拟递归来解决。
-  在快排中，当元素规模小于 4 时，不再递归，使用插入排序。
-  插入排序使用哨兵优化。

### Java 中的基本元素排序

对基本元素类型，使用双轴快排

### Java 中的引用元素排序

在 jdk 8 中，保留一个参数，若是开启，会使用归并排序。
 默认使用 TimSort。大致思路如下：

1. 元素个数 < 32，采用二分查找插入排序(Binary Sort)。
2. 元素个数 >= 32，采用归并排序，归并的核心是分区(Run)。
3. 找出连续升或降的序列作为分区，分区最终倍调整为升序后压入栈。
4. 如果连续分区长度太小，通过二分插入排序扩充长度到分区最小阈值。
5. 每次压入栈，都要检查已存在的分区是否满足合并条件，满足则进行合并。
6. 最终栈内的分区倍全部合并，得到一个排序好的数组。

TimSort 的算法非常巧妙：

1. 找出左分区最后一个元素（最大）及在右分区的位置
2. 找出右分区第一个元素（最小）及在左分区的位置
3. 仅针对这两个位置进行合并，之外的元素元素本身就是有序的。

# 十、二分搜索

​	二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为0。时间复杂度是O(logn)

实现代码：

```java
/**
     * @param a     待查找数组
     * @param n     数组长度
     * @param value 待查找的值
     * @return
     */
    public int bSearch(int[] a, int n, int value) {
        int low = 0;
        int high = n - 1;
        while (low <= high) {
            int mid = low + (high - low) / 2;
            if (a[mid] == value) {
                return mid;
            } else if (a[mid] < value) {
                low = mid + 1;
            } else {
                high = mid - 1;
            }
        }
        return -1;
    }
```

使用场景：

- 数据是通过顺序表结构存储的，比如数组；
- 有序数据，若无序需要先排序
- 数据量需要比较大

## 变形的二分查找

上面的二分查找是最简单的一种，即有序数据集合中不存在重复的数据，我们在其中查找值等于某个给定值的数据。

![image-20190502120336310](/Users/jack/Desktop/md/images/image-20190502120336310.png)

1.在有序重复数组中，查找第一个值等于给定值的元素

```java
public int bSearch(int[] a, int n, int value) {
        int low = 0;
        int high = n - 1;
        // 注意循环退出的条件是low <= high，而不是low < high，因为等于也成立
        while (low <= high) {
//            int mid = low + (high - low) / 2;
            int mid = low + ((high - low) >> 1);
            if (a[mid] > value) {
                high=mid-1;
            } else if (a[mid] < value) {
                low = mid + 1;
            } else {    //a[mid]==value的时
//如果此时mid是第一个元素的话，因为是排序数组，所以肯定是第一个相等的
//如果mid的前一个元素不等于value，那么表明mid也是第一个相等的，所以这两种情况都是返回mid
//如果上面的情况都不符合，那么表明不是第一个相等的，所以将high左移,第一个在左边区域，继续遍历                
                if ((mid==0)||a[mid-1]!=value) return mid;
                else high=mid-1;
            }
        }
        return -1;
    }
```

2.在有序重复数组中，查找最后一个值等于给定值的元素

对上面的else进行修改即可。

```Java
 else {    //a[mid]==value的时
//如果此时mid是最后一个元素的话，因为是排序数组，所以肯定是最后一个相等的
//如果mid的后一个元素不等于value，那么表明mid是最后一个相等的，所以这两种情况都是返回mid
//如果上面的情况都不符合，那么表明不是最后一个相等的，所以将low右移,最后一个相等的在右边区域，继续遍历                
                if ((mid==n-1)||a[mid+1]!=value) return mid;
                else low=mid+1;
            }
```

3.在有序重复数组中，查找第一个大于等于给定值的元素。

思路与上面的两种变形类似。

```Java
public int bSearch(int[] a, int n, int value) {
        int low = 0;
        int high = n - 1;
        // 注意循环退出的条件是low <= high，而不是low < high，因为等于也成立
        while (low <= high) {
//            int mid = low + (high - low) / 2;
            int mid = low + ((high - low) >> 1);
            if (a[mid] >= value) {  // 查看是否为第一个大于value的数
                if ((mid == 0) || a[mid - 1] < value) return mid;
                else high = mid - 1;
            } else {
// 如果a[mid]小于要查找的值value，那要查找的值肯定在[mid+1, high]之间，所以low=mid+1。
                low = mid + 1;
            }
        }
        return -1;
    }
```

4.在有序重复数组中，查找最后一个小于等于给定的元素。

```Java
public int bSearch(int[] a, int n, int value) {
        int low = 0;
        int high = n - 1;
        // 注意循环退出的条件是low <= high，而不是low < high，因为等于也成立
        while (low <= high) {
//            int mid = low + (high - low) / 2;
            int mid = low + ((high - low) >> 1);
            if (a[mid] > value) {
                high=mid-1;
            } else {
                // 查看是否为最后一个小于等于value的数
                if ((mid == n-1) || a[mid +1] > value) return mid;
                else low = mid +1;
            }
        }
        return -1;
    }
```

# 十一、跳表

​	==跳表是通过对链表建立多级索引实现的一种可以在插入、删除、查找、取连续区间数据性能极其卓越的动态数据结构。时间复杂度都是 O(logn)。==Redis中的有序集合(Sorted Set)就是用跳表和双HashMap构成的字典来实现的。

## 理解跳表

![image-20190502235645162](/Users/jack/Desktop/md/images/image-20190502235645162.png)

​	如果对一个长度为 n 的链表，每两个元素取第一个元素建立索引，那么查找元素的时间就可以降低 1/2，再对这层索引，仍每两个元素取第一个元素建立索引，在这一层查找元素的时间就会降低为下一层索引的 1/2，即这一层总的查找时间为原来的 1/2 * 1/2 = 1/4……最终顶层为两个索引元素，整个数据结构的查找时间就降低为 O(logn)，这就是跳表。

或者抽取多个节点提到上层：

![image-20190502235900994](/Users/jack/Desktop/md/images/image-20190502235900994.png)

## 跳表到底有多快

​	假设从底到顶每层索引标记为第 1 层索引，第 2 层索引……每层节点的数量是 n / (2^k)。若顶层有两个节点，那么总共有 log[2]n-1 层索引，包含底层链表，就是 log[2]n 层，又每层需要遍历不超过 3 个节点，总共的查找时间就是 O(3 * logn) = O(logn)。

从这个角度看，我们通过空间换时间，用链表实现了 “二分查找”。

**如果我们在链表中存储的是大对象，我们在索引中只是保存要比较的 key。那这点空间又其实可以忽略。**

## 维护跳表索引

![image-20190503000031692](/Users/jack/Desktop/md/images/image-20190503000031692.png)

​	跳表如果插入元素时索引没有即使地动态建立，那就可能造成索引和原始链表地大小不平衡，导致复杂度退化到链表级别。

​	当我们往跳表中插入数据的时候，我们可以根据随机函数选择将这个数据插入到部分索引层中。比如随机函数生成了值 K，那我们就将这个结点添加到第 1 级到第 k 级的索引之中。

随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。

在跳表中删除元素时，除了要删除链表中的，还要删除索引中的，这就要拿到并保持前驱节点，或者用双向链表。

## 跳表在 redis 中的实现

Redis 中的有序集合是通过跳表和散列表来实现的。有序集合的功能大概有以下几点：

- 插入一个元素
- 查找一个元素-
- 删除一个元素
- 按照区间查找数据
- 迭代输出有序序列

其中，插入、删除、查找、迭代输出有序序列这几个操作，红黑树也可以完成，复杂度一样。但是按照区间查找数据这个功能，红黑树实现的效率就没有跳表高了。

跳表只需要在 O(logn) 时间内定位到区间的开头，然后往后遍历就可以了。

# 十二、散列表

## 散列思想

​	散列表利用的是数组支持下标随机访问，时间复杂度是 O(1) 的特性，**==通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。==**所以散列表其实是数组的一种扩展，由数组演化而来。

​	散列思想就是，通过散列函数，把元素的键值 (key) 映射为数组的下标 (hash(key))，然后将元素存储在数组中对应下标的位置。当我们按照键值 (key) 查找元素时，再利用同样的散列函数，将键值映射为数组下标，从对应下标的位置取数据。

## 散列函数

​	散列函数，顾名思义，它是一个函数，符合 y = f(x) 的特征。我们可以把散列函数定义为 hash(key) , 其中 key 表示元素的键值，hash(key) 表示经过散列函数计算得到的散列值。

散列函数要符合三个基本设计要求：

1. 散列函数集散得到的散列值是一个非负整数;因为数组下标是从 0 开始的。
2. 如果 key1 = key2，那 hash(key1) == hash(key2);相同的 key，经过散列函数映射得到的散列值也应该是相同的，这样我们才能取回存放的元素。
3. 如果 key1 != key2，那 hash(key1) != hash(key2)。这一点是理想情况，但现实要想找到一个不同 key 对应的散列值都不一样的散列函数，几乎是不可能的。因为数组存储的空间有限，而 key 可以无限，所以总有重复的时候，即总会存在散列冲突。

## 散列冲突

**解决散列冲突的方法很多，常用的有两类：开放寻址法和链表法。** Java 中的 LinkedHashMap 就采用了链表法解决冲突，ThreadLocalMap 是通过线性探测的开放寻址法来解决冲突。

### 开放寻址法

​	开放寻址法的核心思想就是，如果出现了散列冲突，我们就再重新探测一个空闲位置，将其插入，比如在hash函数计算出来的位置如果非空置，那么就继续往下遍历寻找。当删除元素时，将该位置的元素标记为 deleted 而不是直接删除。

​	开放寻址法比较适合小数据量，当存在冲突时，速度比链表法快一些，毕竟可以直接利用 CPU 的缓存把整个数组缓存下来， IO 时间快了很多。Java 中的 ThreadLocalMap 就用了开放寻址法。

> 这种实现方式，散列表中的数据都存放在数组中，可以有效利用 CPU 的缓存加快查询速度。序列化过程比较简单。链表法包含指针，序列化就比较麻烦。
>
> 缺点是，删除数据比较麻烦，需要使用特殊标记。所有数据都存在一个数组中，冲突的代价很大。
>
> ==总结，数据量小、装载因子小，适合开放寻址法。这也是 ThreadLocalMap 采用开放寻址法解决散列冲突的原因==。

重新探测新的位置的方法有很多，比如：

- 线性探测
- 二次探测 （二次方探测）
- 双重散列（使用一组散列函数，一个不行换另一个）

### 链表法

​	链表法比开放寻址法更加常用，散列冲突时，将冲突的元素组织成一条链表、或者变种成一棵树。删除元素时可以直接删除。

​	链表法的优点是内存利用率高。链表节点可以在需要的时候再创建，并不需要像开放寻址法那样实现申请好。

​	链表法对冲突的容忍性更高，装载因子可以大于 1，甚至再散列冲突很平均的情况下，即使装载因子达到 10，查找效率也不会大幅度衰退。更进一步，对链表法进行改造，使用红黑树或者跳表解决散列冲突，那即使是极端情况下，所有数据都存放在一个槽内，查询时间也是衰退到 O(logn) 的数量级。

​	缺点是，需要维持链表的指针引用，若是存放小对象，那么指针占用的内存就会对比起来挺多，而且链表法也容易造成内存碎片。

​	==总结起来就是，基于链表法的散列冲突方法适合存储大对象、大数据量的散列表，而且比起开放寻址法，它更加灵活，支持更多的优化策略。==

## 散列表应用

1.实现一个 Word 文档中档次的拼写检查功能

​	常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节，那 20 万个单词大概占用 2MB 的存储空间，就算放大十倍也就是 20MB，对于现在的计算机来说，这个大小完全可以放在内存里面。所以可以利用散列表来储存整个英文单词词典。

​	当用户输入某个英文单词时，我们就拿这个单词取散列表中查找，如果查到，就说明是拼写检查正确的；否者，就提示可能存在拼写错误。

2.假设我们有 10 万条 URL 访问日志，如何按照访问次数给 URL 排序？

​	遍历 10 万条数据，URL 作为 key，value 是访问次数，存入散列表，同时记录下最大的访问次数 K，时间复杂度是 O(n)。==如果 K 不是很大，可以使用桶排序，时间复杂度 O(N)。如果 K 非常大(比如大于 10 万)，就使用快速排序，复杂度 O(NlogN)。==。

3.有两个字符串数组，每个数组大约有 10 万条字符串，如何快速找出两个数组中相同的字符串？

1. 以第一个数组为基础构建一个散列表，字符串作为 key，value 是一个标记。遍历第二个数组，每个字符串作为 key 往散列表中做查询，查不到的字符串存到一个 set 中，遍历结束，这个 set 中就存放着两个数组中相同的字符串。
2. 直接使用 hashset ，若是 add 失败，就是重复的。

散列函数的设计的好坏，决定了散列冲突的概率大小，也直接决定了散列表的性能。

### 好的散列函数的特点

- 散列函数的设计不能太过复杂。

过于复杂的散列函数，会消耗过多的计算时间，也就间接影响到散列表的性能。

- 散列函数生成的值应该尽可能随机且均匀分布

这样才能避免或者最小化散列冲突。即便是出现冲突，散列到每个槽的数据也会分布得比较均匀，不会出现某个槽内的数据特别多的情况。

- 实际工作中还需要综合考虑各种因素。

这些因素有关键字的长度、特点、分布、还有散列表的大小等。

### 常用的散列函数设计方法

- 数据分析法

​        通过分析传入散列函数的 key 的特征规律，设计相应的专用散列函数。比如处理使用散列函数处理手机号码，考虑到手机号码前几位重复性很大，后几位比较随机，我们就可以取手机号码后几位作为散列值。

- 进位相加

​        比如对字符串进行散列，可以将字符串中每个字母的 ASCII 值进行“进位”相加，然后再跟散列表的大小求余、取模，作为散列值。比如，对因为单词 nice 进行散列：
 `hash("nice")=(("n" - "a") * 26*26*26 + ("i" - "a")*26*26 + ("c" - "a")*26+ ("e"-"a")) / 78978`

- 直接寻址法、平方取中法、折叠法、随机数法等

## 装载因子过大了怎么办

​	==装载因子的定义为 `散列表的装载因子 = 填入表中的元素个数 / 散列表的长度`，==根据这个公式，可以推出，装载因子越大且散列表的长度不变，则散列表中的元素越多，空闲位置越少，散列冲突的概率会变大。不仅插入数据的过程要多次寻址或者拉很长的链，查找的过程也会因此变得很慢。

​	对没有频繁插入和删除的静态数据集合来说，我们很容易根据数据的特点、分布等，设计出完美的、极少冲突的散列函数。

对于动态散列表来说，数据集合频繁变动，负载因子可能随着时间不断变大。这时就要动态扩容。

动态扩容的过程，简单来说，就是申请一个新的散列表，然后把原来的数据搬运到新的散列表中，但是不是简单的搬运，而是每个元素都要根据新的散列表重新存储位置。

这个过程，运用平摊分析法，每个插入操作的时间复杂度仍是 O(1)。

扩展一下，当散列表的装载因子小于某个阈值时，我们也可以进行动态缩容。

## 如何避免低效扩容

​	大部分情况下，动态扩容的散列表插入一个数据都很快，但在特殊情况下，装载因子达到扩容的阈值，此时，再插入数据，就会触发漫长的扩容过程，在特定的场合，这样漫长的等待过程是不可接受的。

​	举个直观的例子，假设原来散列表有 1GB 的数据，现在进行扩容，就需要对这 1GB 的数据进行再散列，这个扩容的时间，看起来就很耗时。

​	在这种情况下，“一次性”扩容的机制就不适合了。此时，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子达到扩容的阈值后，我们只申请新空间，但不立即将老的数据全部搬移到新的散列表中。

![image-20190503224322391](/Users/jack/Desktop/md/images/image-20190503224322391.png)

​	当有新的数据插入时，我们就将新数据插入新散列表中，同时从旧散列表中拿出一个数据放入到新的散列表。每次插入一个数据到散列表，我们都重复以上的过程。经过多次操作后，老的散列表中的数据就一点一点全部搬移到新的散列表中了。没有了一次性全部搬移，插入操作就不会出现一次很慢的情况了。

​	**这期间的插入操作，可以先从新的散列表中查，查不到再去旧的散列表中查。甚至，可以在每次查询操作中也穿插一条数据搬移。**

## 工业级散列表HashMap

1.初始大小

HashMap 的初始大小是 16，当然这个默认值是可以设置的，如果事先知道大概的数据量多少，就可以依此设置合适的初始容量。

2.装载因子和动态扩容

​	最大装载因子默认是 0.75。当 HashMap 中元素个数超过 0.75 * capacity 时，就会启动扩容，每次扩容后的容量是原来的两倍。

关于 0.75 这个数字的由来，可以查看这篇文章 <https://blog.csdn.net/reliveIT/article/details/82960063>。

3.散列冲突解决办法

​	HashMap 底层采用链表法解决冲突。在 JDK 1.8 之前，冲突的元素插入链表首端，在 JDK 1.8 之后，插入尾端。另外，在 JDK 1.8 之后，当链表长度超过 8 时，会启动树化，当树中元素少于 6 个时，会退化回链表。

4.散列函数

HashMap 的二次哈希，使用的是除留余数法。

因为 A % B = A & (B - 1)，所以，(h ^ (h >>> 16)) & (capitity -1) = (h ^ (h >>> 16)) % capitity。

## 工业级散列表应该具有的特征

- 支持快速的查询、插入、删除操作
- 内存占用合理，不能浪费过多内存
- 性能稳定，极端情况下也不会性能退化到无法接受

设计这样的散列表应该从三个方面考虑

- 设计合适的散列函数
- 定义合适的装载因子
- 合适的动态扩容策略
- 合适的散列冲突解决办法

## LRU缓存淘汰算法

### 使用散列表+链表实现：

​	我们需要维护一个按照访问时间从大到小有序排列的链表结构。因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将链表头部的结点删除。

​	当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放到链表的尾部;如果找到了，我们就把它移动到链表的尾部。因为查找数据需要遍历链表，所以单纯用链表实现的LRU缓存淘汰算法的时间复杂很高，是O(n)。

> 一个缓存(cache)系统主要包含下面这几个操作: 
>
> 往缓存中添加一个数据; 
>
> 从缓存中删除一个数据;
>
> 在缓存中查找一个数据。

​	这三个操作都要涉及“查找”操作，如果单纯地采用链表的话，时间复杂度只能是O(n)。如果我们将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到O(1)。具体的结构就是下面这个样子:

![image-20190504235140273](/Users/jack/Desktop/md/images/image-20190504235140273.png)

​	我们使用双向链表存储数据，链表中的每个结点处理存储数据(data)、前驱指针(prev)、后继指针(next)之外，还新增了一个特殊的字段hnext。

​	主要作用是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。前驱和后继指针是为了将结点串在双向链表中，**hnext**指针是为了将结点串在散列表的拉链中。

#### LRU查找数据过程：

​	首先，通过散列表，可以在O(1)时间复杂度里在缓存中找到一个数据。当找到数据之后，还需要将它移动到双向链表的尾部。 

​	其次，需要找到数据所在的结点，然后将结点删除。借助散列表，可以在O(1)时间复杂度里找到要删除的结点。因为是双向链表，双向链表可以通过前驱指针O(1)时间复杂度获取前驱结点，**所以在双向链表中，删除结点只需要O(1)的时间复杂度。** 

​	最后，添加数据到缓存稍微有点麻烦，需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部;如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部;如果没有满，就直接将数据放到链表的尾部。

​	这整个过程涉及的查找操作都可以通过散列表来完成。其他的操作，比如删除头结点、链表尾部插入数据等，都可以在O(1)的时间复杂度内完成。所以，这三个操作的时间复杂度都是O(1)。至此，我们就通过散列表和双向链表的组合使用，实现了一个高效的、支持LRU缓存淘汰算法的缓存系统原型。 

## **LinkedHashMap**

​	**LinkedHashMap**是通过双向链表和散列表这两种数据结构组合实现的。**LinkedHashMap**中的**“Linked”**实际上是指的是双向链表，并非指用链表法解决散列冲突。

看下面两个例子：

```JAVA
HashMap<Integer, Integer> m = new LinkedHashMap<>(); 
m.put(3, 11);
m.put(1, 12);
m.put(5, 23);
for (Map.Entry e : m.entrySet()) {
	System.out.println(e.getKey());
}
// 输出3，1，5
```

​	==LinkedHashMap也是通过散列表和链表组合在一起实现的。实际上，它不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据。==

```JAVA
// 10是初始大小，0.75是装载因子，true是表示按照访问时间排序 
HashMap<Integer, Integer> m = new LinkedHashMap<>(10, 0.75f, true); 
m.put(3, 11);
m.put(1, 12);
m.put(5, 23); 
m.put(2, 22);
m.put(3, 26); 
m.get(5);
for (Map.Entry e : m.entrySet()) {
	System.out.println(e.getKey());	
}
//输出1，2，3，5
```

> 解析：
>
> 每次调用put()函数，往LinkedHashMap中添加数据的时候，都会将数据==添加到链表的尾部==，所以，在前四个操作完成之后，链表中的数据是下面这样:
>
> ![image-20190505000406702](/Users/jack/Desktop/md/images/image-20190505000406702.png)
>
> 在第7行代码中，再次将键值为3的数据放入到LinkedHashMap的时候，会先查找这个键值是否已经有了，然后，再将已经存在的(3,11)删除，**并且将新的(3,26)放到链表的尾部**。所以，这个时候链表中的数据就是下面这样:![image-20190505000543876](/Users/jack/Desktop/md/images/image-20190505000543876.png)
>
> 当第8行代码访问到key为5的数据的时候，**将被访问到的数据移动到链表的尾部。**所以，第8代码之后，链表中的数据是下面这样:
>
> ![image-20190505000613850](/Users/jack/Desktop/md/images/image-20190505000613850.png)

​	==实际上，按照访问时间排序的LinkedHashMap本身就是一个支持LRU缓存淘汰策略的缓存系统，实现原理基本一致。==

## 为什么散列表和链表经常会一起使用？

​	因为散列表有 O(1) 的时间查找、删除数据的特性，但是元素是无序的。而链表中的数据可以是有序的，可以顺着指针按顺序遍历所有节点，但是在链表中查找数据的时间是 O(n)。

​	很明显，通过时间换空间，我们同时对一组数据建立链表和散列表两种数据结构，并且组合在一起，就可以构造出一种兼具两者优点的数据结构——快速的插入、查找、删除和按顺序遍历功能。

​	同样的道理，如果我们组合的散列表 + 跳表，我们就可以在上面的基础上额外获得在 O(logn) 的时间内定位元素区间的数据的功能。

## 课后思考

1.今天讲的几个散列表和链表组合使用的例子里，我们用的都是双向链表，如果把双向链表改成单链表，还能否正常工作？为什么呢？

- 对于 LRU 缓存淘汰算法，可以改装成单链表

​	在这个算法里，我们使用链表主要是为了维护有序性（按照元素访问的顺序排列），并不需要逆序输出序列，此时对链表的要求只是删除一个节点，然后把这个节点插入到链表的尾端。我们可以通过特殊的技巧，把下一个节点的数据移到这个节点，然后下个节点来达到类似删除本节点的效果。这个删除时间同样是 O(1)，除了删除的节点是尾节点，就需要 O(n) 的时间找到它的上一个节点。

- 对于需要逆序输出一个区间的数据的情况，最好使用双链表

2.假设猎聘网有 10 万名猎头，每个猎头都可以通过做任务（比如发布职位）来积累积分，然后通过积分来下载简历。假设你是猎聘网的一名工程师，如何在内存中存储这 10 万个猎头 ID 和积分信息，让它能够支持这样几个操作：

- 根据猎头的 ID 快速查找、删除、更新这个猎头的积分信息；
- 查找积分在某个区间的猎头 ID 列表；
- 查找按照积分从小到大排名在第 x 位到第 y 位之间的猎头 ID 列表。

对于这个问题，我们可以维护一个散列表 + 跳表的结构。

​	散列表的 key 存放猎头的 ID，跳表的索引存放猎头的积分，每次猎头的积分发生变化，就从跳表中删除这个节点，再在合适的位置重新插入这个节点，达到维持底层链表有序性的效果。

​	这样实现起来，按照 ID 查找、删除猎头的积分信息的效率是 O(1)，更新积分的效率是 O(logn)，查找积分区间的猎头的时间是 O(logn)。

# 十三、哈希算法

​	哈希算法即将任意长度的二进制值串映射成固定长度的二进制值串，这个映射规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。

## 哈希算法的几点要求

- 单向性：从哈希值不能推出原始数据

- 雪崩性：对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也有接近一半的位数发生变化

- 抗弱碰撞性：散列冲突的概率非常小，之所以不能说是完全不冲突，鸽巢原理解释了这一点。

- 高效性：能快速计算出输入数据的哈希值

## 应用

### 一、安全加密

对于加密的哈希算法来说，有两点格外重要：

- 不可逆推性，不过不必追求绝对不可逆推，只要达到足够的不可逆推要求即可。

- 抗碰撞性，这个要尽可能追求，只要几率极小即可，无法做到绝对，毕竟上面说过了鸽巢原理说明了不存在绝对的两个不同输入不产生相同的输出。

### 二、唯一标识

比如根据哈希值查找库中是否已经有了相同的图片、视频。git 中也使用了 SHA 来标识文件版本和追踪文件。

### 三、数据校验

BT下载的原理是基于P2P协议的。我们从多个机器上并行下载一个2GB的电影，这个电影文件可能会被分割成很多文件块(比如可以分成100块，每块大约20MB)。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。 通过哈希算法，对100个文件块分别取哈希值，并且保存在种子文件中。CRC 算法其实就是一种保证数据安全哈希数据校验。实际中，我们也可以在 BT 协议中使用哈希算法对每块数据进行数据校验，只保存正确数据块。

### 四、散列函数

作为哈希表的散列函数，它直接决定了散列冲突的概率和散列表的性能。

相对于哈希函数的其他应用，散列函数的要求比较低，即使出现个别散列冲突，只要不太严重，也是可以开放寻址法和链表法解决的。

散列函数更关注一组数据能否均匀分布在各个槽中和计算哈希值的效率。

主要是哈希算法在分布式系统中的应用

### 五、负载均衡

​	负载均衡的算法很多，有轮询、随机、加权轮询等。但要实现一个会话粘滞（session sticky）的负载均衡算法就可以使用哈希算法。

如果不适用哈希算法，可以维持一张映射关系表，key 是客户端 IP，value 是服务器编号。实现不难，但弊端是：

- 如果客户端多，那么映射表很大，浪费内存和计算资源
- 客户端上线、下线，服务器扩容、缩容都会导致映射失效，这样维护映射关系的成本很大。

通过哈希算法，对客户端**IP**地址或者会话**ID**计算哈希值，将取得的哈希值与服务器列表的大小进 行取模运算，最终得到的值就是应该被路由到的服务器编号。 这样，我们就可以把同一个IP过来的所有请求，都路由到同一个后端服务器上。 

### 六、数据切片

#### 统计统“搜索关键词”出现的次数

​	假设我们有 1T 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？

难点有两个：1、日志文件大，没法放到一台服务器的内存中。 2、 如果只用一台服务器来处理这么大量的数据，处理时间很长。

处理方法：先对数据进行分片，然后采用多台机器并行处理。

​	我们先从日志文件中，依次读取每条搜索关键词记录，通过哈希函数计算哈希值，再对分片数取模，最终得到的值就是数据应该放入哪个数据片的编号。然后采用多台机器并行处理这些数据片，把结果整合到一起，就是我们最终要的结果了。

实际上，这也是 MapReduce 的基本设计思想。

#### 快速判断图片是否在图库中

​	假设现在有一亿张图片，显然在单机上构建哈希表就不够了。因为单机内存有限，一亿张图片构建出来的哈希表显然远远超出一台普通机器的内存上限。

​	解决方法仍然是对数据进行分片，然后采用多机处理。我们准备 n 台机器，让每台机器维护某一部分图片对应的散列表。我们每次从图库中读取一张图片，计算唯一标识，然后与机器个数 n 求余取模，得到的值就是对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建哈希表。

当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算标识，求余取模，在对应的机器查找。

现在估算一下给 1 亿张图片构建分布式哈希表需要大约多少台机器。

散列表中每个数据单元包含两个信息，哈希值和图片文件的路径。假设采用 MD5 计算哈希值，那长度就是 128 比特，即 16 字节。文件路径长度上限是 256 字节，我们取中间值 128 字节计算。再假设采用链表法解决哈希冲突，每个数据单元加一个 8 字节的指针。总计算下来，每个数据单元占用 152 字节。

假设一台机器的内存是 2 GB，散列表的装在因子为 0.75，那一台机器大约可以给 1000万 （2GB * 0.75 / 152 Byte）张图片构建散列表。所以，如果要对 1 亿张图片构建索引，大约需要十几台机器。

==借助这种分布式处理，分片思路，可以解决海量数据处理的单机内存、CPU 问题。==

### 七、分布式存储

​	当面对海量的数据、用户时，为了提高数据的读取、写入能力，一般可以采用分布式的方式来存储数据，比如分布式缓存。

​	思路仍然是构建一个哈希算法，将不同的数据缓存到不同的机器上。但是，考虑到数据是不断增加的，假设有一天需要进行服务器扩容，那么服务器数量增加，原来的哈希算法计算出来的哈希值就都失效了，需要重新计算正确的缓存服务器来读取、写入数据。这时，所有的请求都会穿透缓存，直达数据库，很容易发生**雪崩效应**，压垮服务器。

​	这时就可以采用**一致性哈希算法**。假设我们有 k 台机器，数据的哈希值范围是 [0, max]。我们将整个范围划分成 m 个小区间 ( m 远小于 k) ，每个机器负责 m/k 个小区间，当有新机器加入时，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据。也保持了各个机器上的数据数量的均衡。

一致性哈希的介绍 <https://www.sohu.com/a/158141377_479559>

# 十四、二叉树

## 树

- 根节点：没有父节点的节点
- 叶节点：没有子节点的节点
- 节点的高度：节点到叶子节点的最长路径，从 0 开始
- 节点的深度：根节点到这个节点所经历的边的个数，从 0 开始
- 节点的层数：节点的深度 + 1
- 树的高度：根节点的高度

![image-20190511230746164](/Users/jack/Desktop/md/images/image-20190511230746164.png)

## 二叉树

存储一棵二叉树，有两种办法，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。

- 满二叉树：除了叶子节点之外，其他节点都有左右两个孩子节点。
- 完全二叉树：叶子节点都在最下面两层，最后一层的叶子节点都靠左排列，除了最后一层的节点，其他层的节点都要排满。

完全二叉树天然适合用数组存储，若是有哨兵位置，如果每个节点 X 存储在数据下标为 i 的位置上，则每个节点若是有左儿子，则左儿子在 2 * i 位置上，若是有右儿子，右儿子必在 2 * i + 1 的位置上。

![image-20190511231438405](/Users/jack/Desktop/md/images/image-20190511231438405.png)

从整个数组来看，空间被充分利用起来（除了哨兵位置，但是哨兵位置带来的优化也是很不错的），利用了数组的优点，访问父子节点的速度快很多，还天然支持按层遍历。

### 二叉树的遍历

- 前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。
- 中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。
- 后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。

![image-20190511231755939](/Users/jack/Desktop/md/images/image-20190511231755939.png)

**二叉树遍历的时间复杂度是O(n)。**

## 课后思考

什么样的二叉树适合用数组存储

最完美的情况当然是完全二叉树。但是我思考了下，如果我们的业务场景中，构造的树缺少了极个别的节点，不能构成完全二叉树，但是非常接近完全二叉树，我认为还是可以用数组存储的。毕竟我们做工程，不是要追求学术上的某一指标绝对最优，整体最优才是我们更应该考虑的。

给定一组数据，比如 1，3，4，5，9，10。可以构建出多少种不同的二叉树？

根据卡特兰数C[n,2n] / (n+1)计算，C(6) = c[12,6]/7 = 132 种

## 二叉查找树

**二叉查找树最大的特点就是，支持动态数据集合的快速插入、删除、查找操作。** 

​	二叉查找树又叫二叉搜索树。特点是，在树中任意一个节点，其左子树的每个节点的值，都要小于这个节点的值，而右节点的值都大于这个节点的值。

1. 查找

从根节点递归，注意判断 null，对当前节点进行比较，等于则返回，小于则往左递归，否则往右递归。

1. 插入

思路类似查找操作，遇到 null 则应该直接插入。如果有相同的节点，考虑使用链表法解决，或者选定一个方向插入，比如插入左子树最右边，或则插入右子树最左边。这时树左右子树同当前节点的关系就变成了不大于、不小于的关系了，而原来的关系是小于、大于关系。这种情况下，查找操作和删除操作也应该做相应改变。

1. 删除

仍然类似查找操作，遇到 null 返回 false。最简单的删除操作应该就是这样：

- 没有孩子，直接结束。
- 只有一个孩子，孩子替换到当前位置
- 两个孩子，将删除节点的左孩子移到被删除节点的位置，将左孩子的右子树插入右孩子的最左边，然后把右子树替换在左子树的右儿子位置。

1. 输出有序序列

按中序遍历输出即可。

1. 复杂度分析

最好情况下，树的高度小于等于 log2n，其中 n 是节点个数。此时查找、插入、删除操作的时间复杂度是 O(log2n)。最坏情况下，时间复杂度是 O(n)，此时树退化成链表。

## 有了高效的散列表，为什么还需要二叉查找树呢

我觉得除了历史原因——跳表比平衡树出现得晚以外，平衡树已经有了成熟的方案保证极端情况下的性能不会退化到明显的程度。而散列表、跳表等结构，还是可能随着插入删除等操作的进行，极端情况下有数量级的性能退化的。

总的来说，有以下一些原因：

- 散列表中的数据是无序存储的，若要输出有序数据，需要进行排序。而二叉搜索树可以用中序遍历在 O(n) 时间内完成。
- 散列表扩容耗时很多，遇到散列冲突，性能不稳定。虽然二叉搜索树性能不稳定，但是它的优化版，平衡树的性能一直很稳定。
- 尽管散列表查找的性能是 O(1) 级的，但是因为散列冲突的存在，和哈希函数的耗时，性能不一定比 O(logn) 级快，毕竟 logn 是一个很小的数字。
- 散列表的构造要考虑的点很多，比如散列函数的设计、冲突解决的方法、初始容量和负载因子、扩容、缩容等。而平衡树就不需要考虑那么多了，唯一的问题平衡性已经有成熟的方案了。

但，要说散列表比树的优点，也是很多。。这两种数据结构不应该是有绝对的优劣的，应该根据具体场景具体选择。

## 求给定一颗二叉树的确切高度

1. 递归法

这种方法最简单好理解。 树高 = 根节点高度 = max(leftHeight, rightHeight) + 1

1. 队列法

类似层次遍历，维持一个队列，同时设变量记录下当前层数，当前层剩余未遍历个数，下一层个数。遍历完这个队列即可得到树的高度，也避免了过大的内存开销和过深的递归可能出现的问题。















参照：https://www.jianshu.com/p/f1eaf1e8e073

《数据结构与算法之美》、<https://www.jianshu.com/nb/29712522>
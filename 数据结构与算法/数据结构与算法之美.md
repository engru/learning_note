# 数据结构与算法之美

# 一、时间复杂度

### 1.概述

​	**代码执行时间 T(n)=O(f(n))**

其中 T(n) 表示代码执行的时间，n 表示数据规模的大小，f(n) 表示每行代码执行的次数总和。

其中 大 O 时间复杂度实际上并不具体表示代码执行的时间，而是表示代码执行时间随数据规模增长的变化趋势，，所以，也叫渐近时间复杂度，简称时间复杂度。

1. **只关注执行循环最多的一段代码，总复杂度等于量级最大那段代码的复杂度**

2. 嵌套代码的复杂度等于嵌套内外代码复杂度的乘积。

​	时间复杂度表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。

### 2.几种常见的复杂度：

常量阶 O(1)、对数阶 O(logn)、线性阶 O(n)、线性对数阶 O(nlogn)、平方阶 O(n^2)、立方阶 O(n^3)、K 次方阶 O(n^k)、指数阶 O(2^n)、阶乘阶 O(n!)

对这些复杂度量级，可以分为多项式量级和非多项式量级。其中非多项式量级只有指数阶和阶乘阶，这两个量级的算法问题又叫做 NP 难问题。

#### 1.O(1)

​	只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)。或者说，一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。

#### 2. O(logn)、O(nlogn)

​	当已处理的数据规模呈等比数列级别时，代码的复杂度就是对数级别 O(logn)。而 O(nlogn) 和 O(logn) 联系其实很紧密，将 O(logn) 级别的代码执行 O(n) 级别的次数，就是 O(nlogn) 了。归并排序、快速排序的时间
复杂度都是 O(nlogn)。

#### 3.对 O(m+n)、 O(m*n) 的理解

​	之所以有两个参数，是因为代码的复杂度由两个数据的规模决定。因为无法实现知道 m 和 n 两个数据规模谁更大，所以只能都写出来。

![image-20190215170051760](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190215170051760.png)

### 3.复杂情况下的时间复杂度

​	为了表示代码在不同情况下的不同时间复杂度，我们需要引入三个概念**:最好情况时间复杂度、**
**最坏情况时间复杂度和平均情况时间复杂度。**

1.最坏情况时间复杂度:代码在最理想情况下执行的时间复杂度。
2.最好情况时间复杂度:代码在最坏情况下执行的时间复杂度。
3.平均时间复杂度:用代码在所有情况下执行的次数的加权平均值表示。
4.均摊时间复杂度:在代码执行的所有复杂度情况中绝大部分是低级别的复杂度，个别情况是
高级别复杂度且发生具有时序关系时，可以将个别高级别复杂度均摊到低级别复杂度上。基
本上均摊结果就等于低级别复杂度。

# 二、数组

## 概念

1. 数据是一种**线性表**数据结构，所谓线性表，就是数据排成像一条线一样的数据结构。
2. 这里数组用一组**连续的内存空间**，来存储**相同类型的数据**。

![image-20190215234850951](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190215234850951.png)

![image-20190215234905780](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190215234905780.png)

数组支持**随机访问**。这个特点也是因为它占有连续的内存空间。

我们拿一个长度为 10 的 int 类型的数组 int[] a = new int[10] 来举例。在我画的这个图中，计算机给数组 a[10]，分配了一块连续内存空间 1000~1039，其中，内存块的首地址为base_address = 1000。

![image-20190215235021439](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190215235021439.png)

​	计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址:

```
a[i]_address = base_address + i * data_type_size
```

​	其中 data_type_size 表示数组中每个元素的大小。我们举的这个例子里，数组中存储的是 int类型数据，所以 data_type_size 就为 4 个字节。

**二维数组内存寻址:**
对于 m * n 的数组，a [ i ][ j ] (i < m,j < n)的地址为:

```
address = base_address + ( i * n + j) * type_size
```

当面试时，我们不应该说数组的查找时间复杂度是 O(1)，排序好的数组，用二分查找，时间复杂度是 O(logn)。正确的表述是，根据下标随机访问的时间复杂度是 O(1)。

但是也是因为这个原因，数组的插入和删除非常“低效”。为了保持连续性，需要做大量的数据迁移工作。

## 插入

如果数据是有序的，每次插入到数组的第 k 个位置，需要把 k~n 这部分数据都往后移以为，若是在每个位置插入元素的概率是一样的，那么平均时间复杂度是 (1+2+...n)/n=O(n)。

若数据是无序的，数组只是一个存储数据的集合，这种情况下，要把数据插入到第 k 个位置，可以尝试把第 k 个元素移到数组的最后面，把新元素插入到第 k 个位置，这样在特定场景下，插入一个元素到第 k 个位置时间复杂度可以降为 O(1)。

## 删除

和插入一样，最好情况下时间复杂度是 O(1)，如果删除开头的数据，则是最坏情况时间复杂度 O(n)，平均情况下时间复杂度是 O(n)。

如果我们将多次删除操作集中在一起删除，就可以提高删除的效率，这也是 jvm 的标记清楚垃圾回收算法。

## 容器

ArrayList 相比数组，最大的优势就是将许多细节封装起来了，比如前面提到的数组插入、删除时需要搬移其他数据等。另外的优势就是自动扩容了。

但不是所有情况都需要用到 ArrayList。比如

1. ArrayList 无法存储基本类型。自动封箱拆箱需要性能消耗。
2. 有些操作较为简单，无需用到 ArrayList。
3. 定义多维数组时，若是用 ArrayList 看起来不直观。

![image-20190215235559326](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190215235559326.png)

# 三、链表

​	缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非常广泛的应用，比如常 见的 **CPU 缓存、数据库缓存、浏览器缓存等等。** 

​	缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留?这就需要 缓存淘汰策略来决定。**==常见的策略有三种:先进先出策略 FIFO(First In，First Out)、最少使 用策略 LFU(Least Frequently Used)、最近最少使用策略 LRU(Least Recently Used)。==** 

## 数组和链表的内存比较

![image-20190217105102418](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217105102418.png)

​	从图中我们看到，数组需要一块连续的内存空间来存储，对内 存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储 空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。 

而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起 来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。 

​	链表是通过“指针”将一组零散的内存块串联起来的数据结构，相比数组就是使用一组连续的内存块来存储数据的数据结构。

常见的链表有：

-  **单链表**： 每个数据块只有一个指针指向下一节点的数据。其中有两个特殊的节点，一个是头节点，数据块是空的，只有一个指针指向下一节点；另一个节点是尾节点，他的特点是，指针是指向空地址 NULL，表示这是链表的最后一个节点。![image-20190217105145325](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217105145325.png)
-  **循环链表**： 循环链表是，每个数据块都有一个指针指向下一节点，尾节点的指针指向头节点，另有一个单独的头指针指向开头。![image-20190217105345482](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217105345482.png)
-  **双链表**： 双链表是在单链表的基础上，**在每个数据块节点上增加一个指针指向上一节点。**每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针prev 指向前面的结点。![image-20190217105948132](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217105948132.png)
-  **双向循环链表**：双向循环链表是每个数据块都有两个指针，一个向前指向，一个向后指向，另有一个单独的指针指向一个数据块节点，这个指针是头指针。![image-20190217110211704](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217110211704.png)

## 性能

​	==链表的随机访问性能是 O(n),数组是 O(1)。链表的插入、删除操作的性能是 O(1)，数组是 O(n)。==

​	这里的链表，插入，删除，是指知道要插入的点，删除的点的指针，比如单链表插入时知道插入点上一节点的指针时，通过改变指针的指向就可以完成 O(1) 时间内的数据插入，删除同理。

​	若是单链表删除时，不知道指向要删除的具体节点的指针，那么就要在删除前先进行随机访问，那么性能就是 O(n)。

​	对于有序双向链表，查询效率会比单链表高一点，因为我们可以记录上次查找的位置 P，每次查询时，根据要查询的值与 P 的大小关系，决定往前还是往后查找，所以平均只需要查找一般的数据。

**在 Java 语言中，LinkedHashMap 这个容器，他的实现原理就用到了双向链表这个数据结构。**

## 数组 VS 链表

​	数组简单易用，在实现上是使用连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中不是连续存储，对 CPU 缓存不友好，没办法有效预读。

​	**数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足(out of memory)”。**如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。	

​	但数组的特点也是它的不足，他的内存空间是固定的，如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致内存不足(out of memory)，例如如果现在系统由 100M 不连续的内存空间，声明 100M 数组就会失败，另外到数组扩容时，复制原数组的内容到新数组也很费时。这就是数组和链表最大的区别。

另外如果我们的代码对内存使用非常苛刻，那应该使用数组，比如安卓之类的，因为链表需要维护额外的空间去存储指针。而且对链表进行频繁的插入、删除操作，还对导致频繁的内存申请和释放，容易造成内存碎片。如果是 Java 语言，就可能导致频繁的 gc。![image-20190217110226500](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217110226500.png)

## 如何书写正确的链表代码

### 技巧一：理解指针或引用的含义

​	**将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向这个变量，通过指针就能找到这个变量。**

如：在编写链表代码的时候，我们经常会有这样的代码:p->next=q。这行代码是说，p 结点中的 next 指针存储了 q 结点的内存地址。 

还有一个更复杂的，也是我们写链表代码经常会用到的:p->next=p->next->next。这行代码 表示，p 结点的 next 指针存储了 p 结点的下下一个结点的内存地址。 

### 技巧二：警惕指针丢失和内存泄漏

​	写链表时，一定要注意指针指向哪了，对于脑子转不过来的情况，可以在纸上画图辅助思考。
 对于自己管理内存的语言，如 C 语言，如果没有手动释放节点对应的内存空间，就会产生内存泄漏。不过，对于 Java 这种虚拟机自动管理内存的编程语言来说，就不需要考虑那么多了。

### 技巧三：利用哨兵简化实现难度

​	有时代码写不出来，也是因为代码的小逻辑多而乱，如果能够实现分析代码，简化逻辑，那么写起代码来就会更容易轻松了。
 比如，**当在单链表插入一个新节点时，需要两个小逻辑：1. *链表是空的的情况* 2. *链表不是空的情况* 写起来的代码就会像这样复杂。而如果我们有一个哨兵节点，那么就只需“无脑”往这个节点后面插入新节点而不用进行一次判空特殊处理了。**

其实就是加了一个虚拟头节点，这个结点是不存储数据的。因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了。

**拓展**：==在很多算法中都有用到哨兵做简化，比如插入排序、归并排序、动态规划等==。
 下面这个例子就是用了哨兵提升性能：

```Java
inf find(char* a, int n, int key) {
  if (a[n-1] == key) {
    return n-1;
  }
  char tmp = a[n-1];
  a[n-1] = key;
  int i = 0;
  while (a[i] != key) {
    ++i;
  }
  a[n-1] = tmp;
  if (i == n-1) return -1;
  return i;
}
```

### 技巧四：重点留意边界条件处理

软件开发中，我们往往是从“通常情况入手，设计代码”，这种情况下，如果不注意特殊情况（边界情况）时，就容易产生 BUG。一定要在写完代码后，检查边界条件是否考虑齐全。
 常用来检查链表代码是否正确的边界条件有这样几个：

- 如果链表为空时，代码能否正常工作？
- 如果链表只包含一个节点时，代码能否正常工作？
- 如果链表只包含两个节点时，代码能否正常工作？
- 代码逻辑在处理头节点和尾节点时，能否正常工作？

### 技巧五：画图举例，辅助思考

感觉这也是软件设计图的细节版。写出来，整理一下，就明白了。![image-20190217120701097](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217120701097.png)

### 技巧六：多写多练，没有捷径

#### 5个常见的链表操作：

- 单链表反转
- 链表中环的检测
- 两个有序的链表合并
- 删除链表倒数第 n 个结点
- 求链表的中间结点

对应LeetCode题目：206，141，21，19，876

# 四、栈

栈，是一种“操作受限”的线性表，只允许在一端插入和删除数据。后进后出是它的最大特点。

## 栈的实现

​	栈可以用数组和链表实现。数组实现的栈叫顺序栈，链表实现的栈叫链式栈。特别要注意的是，顺序栈的动态扩容，应用平摊分析法，最终分析出插入的时间复杂度仍是 O(1)。![image-20190217131404224](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217131404224.png)

## 栈的应用

1. **函数调用栈**
    操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构，用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈。**当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。![image-20190217150150940](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217150150940.png)**![image-20190217150202171](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217150202171.png)
2. **栈在函数表达式中的应用**
    编译器就是通过两个栈来实现表达式的运算的，一个保存操作数的栈，另一个保存运算符的栈。我们从左向右遍历表达式，**当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶符号比较。如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，**从操作数的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。
   1. 例子：3+5*8-6![image-20190217151249246](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190217151249246.png)
3. **栈在括号匹配中的应用**
    **我们用栈来保存未匹配的左括号**，从左到右依次扫描字符串。**当扫描到左括号时，则讲其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。**
    当所有括号都扫描完成之后，如果栈为空，则说明字符串是合法格式；否则，说明有未匹配的左括号，为非法格式。

## 课后思考

1. **我们在讲栈的应用时，讲到用函数调用栈来保存临时变量，为什么函数调用要用“栈”来保存临时变量呢？其他数据结构不行吗？**

特定数据结构是特定应用场景的抽象。

函数调用的顺序，符合先进者后出，后进者先出的特点，还有函数中调用函数，也是这样，先开始执行的函数，必须等到内部调用的其他函数执行完毕，该函数才能执行结束。

比如函数中的局部变量的生命周期的长短，是先定义的局部变量生命周期长，后定义的局部变量生命周期短。

1. **我们都知道，JVM 内存管理中有个“堆栈”的概念。栈内存用来存储局部变量和方法调用，堆内存用来存储 Java 中的对象。那么 JVM 里面的“栈”跟我们这里说的“栈”是不是一回事呢？如果不是，那它又为什么叫做“栈”呢？**

内存中的堆栈和数据结构堆栈不是一个概念。内存中的堆栈是真实存在的物理区，数据结构中的堆栈是抽象的数据结构。
 内存空间在逻辑上分为三部分：代码区、静态数据区和动态数据区。动态数据区又分为栈区和堆区。
 代码区：存储方法体的二进制代码。高级调度（作业调度）、中级调度（内存调度）、低级调度（进程调度）控制代码区执行代码的切换。
 静态数据区：存储全局变量、静态变量、常量，常量包括 final 修饰的常量和 String 常量。系统自动分配和回收。
 栈区：存储方法的形参、局部变量、返回值。由系统自动分配和回收。
 堆区：new 操作符创建的一个对象的引用地址存储在栈区，指向该对象存储在堆区中的真实数据。

# 五、队列

队列也是一种“操作受限”的线性表，只支持两种基本操作：入队和出队。

队列的应用非常广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。它们在很多偏底层的系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等。

​	**队列有两个指针，一个head指针和一个tail指针，分别指向队头和队尾，当插入元素的时候，即入队的时候，tail指针向右移动，head指针不变；当出队的时候，head指针向右移动，tail指针不变**

![image-20190224230644568](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190224230644568.png)

​	当我们调用两次出队操作之后，队列中 head 指针指向下标为 2 的位置，tail 指针仍然指向下标为 4 的位置。![image-20190224230659386](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190224230659386.png)

​	当队列的 tail 指针移动到数组的最右边后，如果有新的数据入队，我们可以将 head 到 tail 之间的数据，整体搬移到数组中 0 到 tail-head 的位置。![image-20190224230807069](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190224230807069.png)

## 顺序队列和链式队列

用数组实现的队列叫顺序队列，用链表实现的队列叫链式队列。

==顺序队列在没有空闲空间时，需要触发一次数据的搬移操作==。

## 循环队列

![image-20190224231004881](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190224231004881.png)

​	图中这个队列的大小为 8，当前 head=4，tail=7。当有一个新的元素 a 入队时，我们放入下标为 7 的位置。但这个时候，我们并不把 tail 更新为 8，而是将其在环中后移一位，到下标为 0 的位置。当再有一个元素 b 入队时，我们将 b 放入下标为 0 的位置，然后tail 加 1 更新为 1。所以，在 a，b 依次入队之后，循环队列中的元素就变成了下面的样子:![image-20190224231118349](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190224231118349.png)

​	将顺序队列首尾连接起来就是循环队列。要注意的点就是，队列判满的条件是==(tail + 1) % n = head==。如果 head == tail 表示队列为空。

队满的情况：

![image-20190224231304271](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190224231304271.png)

​	当队列满时，图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间。

```java
public class CircularQueue {
    // 数组:items，数组大小:n
    private String[] items;
    private int n = 0;
    // head 表示队头下标，tail 表示队尾下标
    private int head = 0;
    private int tail = 0;

    // 申请一个大小为 capacity 的数组
    public CircularQueue(int capacity) {
        items = new String[capacity];
        n = capacity;
    }

    //入队
    public boolean enqueue(String item) {
        // 队列满了
        if ((tail + 1) % n == head) return false;
        items[tail] = item;
        tail = (tail + 1) % n;
        return true;
    }

    //出队
    public String dequeue() {
// 如果 head == tail 表示队列为空
        if (head == tail) return null;
        String ret = items[head];
        head = (head + 1) % n;
        return ret;
    }
}
```

## 阻塞队列和并发队列

​	**阻塞队列**就是在队列的基础上增加了阻塞操作。简单来说，就是在队列为空的时候，出队操作会被阻塞，当队列满的时候，入队操作会被阻塞。其实，上面的定义，就是一个“**生产者-消费者**”模型。

​	这种模型，可以有效协调生产和消费的速度。当“生产者”生产速度过快，“消费者”来不及消费时，存储队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续生产。

​	而且不仅如此，基于阻塞队列，我们还可以协调“生产者”和“消费者”的个数，来提高数据处理的效率，比如，我们可以多配置几个“消费者”，来对应一个“生产者”。

**并发队列**：线程安全的队列我们叫做并发队列。最简单的实现方式就是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低。实际上，基于 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。

## 队列在线程池中的应用

当线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？

我们一般有两种处理策略一种是非阻塞队列，一种是阻塞队列。

阻塞队列又有两种实现方式，一种是无限排队等待，另一种就是基于数组的有限队列，这里，给数组队列设置一个合理的队列大小，是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。

## 基于链表和数组的队列的区别

基于链表的实现方式，可以实现一个支持**无限排队的无界队列(unbounded queue)**，但是可 能会导致过多的请求排队等待，请求处理的响应时间过长。所以，针对响应时间比较敏感的系 统，基于链表实现的无限排队的线程池是不合适的。 

而基于数组实现的有界队列(bounded queue)，队列的大小有限，所以线程池中排队的请求 超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。不过，设置一个合理的队列大小，也是非常有讲究的。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。

==可以使用 cas + 数组的方式实现无锁并发队列。分布式消息队列，如 kafka 也是一种队列。==

# 六、递归

## 如何理解递归

个人觉得递归就是“递” + “归” + 栈。生成子问题是递的过程，这个过程是把函数和变量压入栈，从子问题返回母问题的过程是归，这个过程把函数和变量从栈中取出。

## 递归的三个条件

1. 一个为题的解可以分解为几个子问题的解。
2. 这个问题和分解之后的子问题，除了数据规模不同，求解思路完全一样。
3. 存在递归终止条件。

## 如何编写递归代码

**写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。**

另外，编写递归代码还有一个关键，那就是遇到递归，就把它抽象成一个递推公式，不用想一层一层的调用关系，不要试图用人脑去分解递归的每个步骤。

## 递归要注意的点

- **递归代码要警惕堆栈溢出**

函数调用使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，就会有堆栈溢出的风险。

我们可以通过在代码中限制递归调用的最大深度的方式来解决这个问题。递归超过一定深度后就不继续往下递归了。

- **递归代码要警惕重复计算**

如果递归分解出来的子问题很多都是重复计算的话，可以考虑通过一个数据结构（比如散列表）来保存求解过的结果。当递归调用到求解过的结果时，直接返回求解过的结果即可。

- **时间和空间成本**

在时间效率上，递归代码里有很多函数调用，这些函数调用的数量较大时，就会积聚成一个可观的成本。

在空间效率上，递归调用一次就会在内存栈中保存一次现场数据，这部分也会占用一定的空间。

## 怎么改递归代码为非递归代码

因为递归本身就是借助栈来实现的，只不过我们使用的栈是系统或者虚拟机本身提供的，我们没有感知罢了。如果我们在自己的内存堆上实现栈，手动模拟入栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样子。不过这样仍然不能避免递归的某些弊端，而且徒增了递归的复杂度。

另一种正常一点的改写成非递归代码的方法是，根据初始条件和递归关系，手动用循环模拟每一层的操作。

## 课后思考

我们平时调试递归代码喜欢使用 IDE 的单步跟踪功能，像规模较大、递归层次很深的递归代码，几乎无法使用这种调试方式。对于递归代码，你有什么好的调试方式呢？

1. 打印出关键日志
2. 设置条件断点，进入特定层次位置。

另外，调试递归代码的方式，同样适合调试循环轮次较多的情况。

# 七、排序

![image-20190225153746922](/Users/jack/Desktop/md/images/image-20190225153746922.png)

## 1.如何分析排序算法

### 算法的执行效率

1. **最好情况、最坏情况、平均时间复杂度**

要了解这三种情况分别对应什么样的原始数据输入。

 2.**时间复杂度的系数、常数、低阶**

​	时间复杂度是当数据规模 n 很大时的一个增长趋势，所以可以忽略系数、常数、低阶。
 但在实际软件开发中，我们可能要排序的是 10 个、100 个、 1000 个这样的小规模数据。这时我们就要分析每个整个 T(n），而不仅仅是 O(n) 了。

3.**比较次数和交换（或移动）次数**

​	排序算法分为基于比较的算法，还有不基于比较的算法。如果我们分析基于比较的算法，那就同时把这两种操作：比较和交换 一起考虑。

​	比如我们有一组数据2，9，3，4，8，3，按照大小排序之后就是2，3，3，4，8，9。 这组数据里有两个3。经过某种排序算法排序之后，如果两个3的前后顺序没有改变，那我们就把这种排序算法叫作稳定的排序算法;如果前后顺序发生变化，那对应的排序算法就叫作不稳定的排序算法。

​	比如说，**我们现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额。**如果我们现在有10万条订单数据，我们希望按照 金额从小到大对订单数据排序。对于金额相同的订单，我们希望按照下单时间从早到晚有序。

> ​	最先想到的方法是:我们先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据，对于每个金额相同的小区间再按照下单时间排序。这种排序思路理解起来不难，但是实现起来会很复杂。
> ​	借助稳定排序算法，这个问题可以非常简洁地解决。解决思路是这样的:我们先按照下单时间给订单排序，注意是按照下单时间，不是金额。排序完成之后，我们用稳定排序算法，按照订单金额重新排序。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。
> ​	因为稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。第一次排序之后，所有的订单按照下单时间从早到晚有序了。在第二次排序中，我们用的是稳定的排序算法，所以经过第二次排序之后，相同金额的订单仍然保持下单时间从早到晚有序。

### 算法的内存消耗

算法的内存消耗应该被考虑。不过还可以了解一个概念：**原地排序特指空间复杂度是 O(1) 的排序算法。**

### 排序算法的稳定性

排序算法的稳定性指的是，如果待排序列中存储了相同的元素，那么经过排序之后，相等的元素之间原有的先后顺序不变。

## 2.O(n2) 的排序算法

### 冒泡排序

​	**冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻两个元素进行比较，看是否满足大小关系要求，如果不满足就让它俩互换。**一次冒泡排序会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再进行后续的冒泡操作。

```Java
// 冒泡排序，a 表示数组，n 表示数组大小
public void bubbleSort(int[] a, int n) {
  if (n <= 1) return;
 
 for (int i = 0; i < n; ++i) {
    // 提前退出冒泡循环的标志位
    boolean flag = false;
    for (int j = 0; j < n - i - 1; ++j) {
      if (a[j] > a[j+1]) { // 交换
        int tmp = a[j];
        a[j] = a[j+1];
        a[j+1] = tmp;
        flag = true;  // 表示有数据交换      
      }
    }
    if (!flag) break;  // 没有数据交换，提前退出
  }
}
```

### 插入排序

插入排序的过程，就是不断将一个新的元素插入到一个有序的数组的过程。

```java
// 插入排序，a 表示数组，n 表示数组大小
 /**
 * 首先对数组的前两个数据进行从小到大的排序。
 * 接着将第三个数据与排好序的两个数据比较，将第三个数据插入合适的位置。
 * 然后将第四个数据插入到已排好序的前3个数据中。
 * 其实就是每次都是一个区域内的排序比较大小
 **/
public void insertionSort(int[] a, int n) {
  if (n <= 1) return;

  for (int i = 1; i < n; ++i) {
    int value = a[i];
    int j = i - 1;
    // 查找插入的位置
    for (; j >= 0; --j) {
      if (a[j] > value) {
        a[j+1] = a[j];  // 数据移动
      } else {
        break;
      }
    }
    a[j+1] = value; // 插入数据
  }
}
```

### 选择排序

​	选择排序和插入排序类似，也是区分已排序区间和未排序区间。但是选择排序每次是选择未排序区间中最小的元素，将其添加到已排序列的末尾。而选择排序是固定选择第一个未排序的元素插入到已排序序列的合适位置。

```java
  public static void selectionSort(int[] a, int n) {
    if (n <= 1) return;
    for (int i = 0; i < n - 1; ++i) {
      // 查找最小值
      int minIndex = i;
      for (int j = i + 1; j < n; ++j) {
        if (a[j] < a[minIndex]) {
          minIndex = j;
        }
      }

      // 交换
      int tmp = a[i];
      a[i] = a[minIndex];
      a[minIndex] = tmp;
    }
  }
```

![image-20190407105104276](/Users/jack/Desktop/md/images/image-20190407105104276.png)

### 2.1 三个问题

#### 是原地排序算法吗？

​	这三个排序都是在原数组上进行排序排序，只需要常量级的临时空间，所以它们的空间复杂度是 O(1)，是原地排序算法。

#### 是稳定的排序算法吗？

- 在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。

冒泡排序和插入排序是稳定的排序算法，选择排序不是稳定的排序算法。

冒泡排序不对相同的元素做交换。插入排序把值相同的元素，将后面出现的元素插入到前面的元素后面，所以也是稳定的算法。

选择排序会找出未排序元素中的最小值，和未排序元素的第一个做交换，这个交换是不稳定的。

### 2.2 时间复杂度是多少？

#### 1.有序度，逆序度，满有序度

​	有序度是数组中具有有序关系的元素对的个数。完全有序的数组，有序度就是 n*(n-1)/2，它的有序度叫满有序度。逆序度跟有序度相反。

==总结公式就是 逆序度 = 满有序度 - 有序度==

#### 2.冒泡排序

​	冒泡排序最好情况下时间复杂度是 O(n)，即要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了。

最坏情况的时间复杂度是 O(n2)，这时要排序的数组是逆序的，需要进行 n 次冒泡操作。

对于冒泡排序的时间复杂度分析如下：冒泡排序每次交换，减少一个逆序度。故冒泡排序的具体每次执行的时间跟输入数据的逆序度息息相关，假如输入一个完全有序的数组，那么这个数组的逆序度就是 0，只需要进行 0 次交换，假如输入的数组是一个完全逆序的数组，那么它的逆序度就是 n*(n-1)/2，需要进行 n*(n-1)/2 次交换。我们不严谨地取个中间值 n*(n-1)/4，表示初始有序度不是很高也不是很低的平均情况。按照这样，平均时间复杂度就是 O(n2)。

1. 插入排序

插入排序最好的时间复杂度是 O(n)，即要排序的数组已经有序，我们从尾到头每次只需要比较一个数据就可以确定插入的位置。

插入排序最坏的时间复杂度是 O(n2)，即待排序的数组完全逆序，每次插入都相当于在数组的第一个位置插入新的数组，这个插入的时间复杂度我们在数组那节分析过，是 O(n)，又一共有 n 个元素，所以最终的复杂度就是 O(n2)。

同样，我们在数组那节分析过，在数组插入一个数据的平均时间复杂度是 O(n)，所以，对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以平均时间复杂度是 O(n2)。

1. 选择排序

选择排序最好、最坏、平均时间复杂度都是 O(n2)。

选择排序每次需要比较整个未排序数组的的所有元素，找出最小的元素插入到已排序数组的最末端。这样每轮的时间复杂度是 O(n)，循环执行 n 次，时间复杂度就是 O(n2)。不受数组排列顺序的影响。

### 2.3 相关问题

**冒泡排序和插入排序的时间复杂度都是 O(n2)，都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎？**

> ​	我们在前面分析过，冒泡排序不管怎么优化，元素交换的次数是一个固定的值，是原始数据的逆序度。插入排序是同样的，不管怎么优化，元素移动的次数等于原始数据的逆序度。
>
> ​	但是，仔细分析下，冒泡排序的交换操作要比插入排序的移动操作复杂得多，==冒泡排序每次需要做三个赋值操作，而插入排序只需要做一个移动操作。==

```java
冒泡排序中数据的交换操作：
if (a[j] > a[j+1]) { // 交换
   int tmp = a[j];
   a[j] = a[j+1];
   a[j+1] = tmp;
   flag = true;
}

插入排序中数据的移动操作：
if (a[j] > value) {
  a[j+1] = a[j];  // 数据移动
} else {
  break;
}
```

​	如果不严谨地把每次操作地时间粗略估计为单位时间 (unit_time)，然后分别用冒泡排序和插入排序对同一个逆序度为 K 的数组进行排序。用冒泡排序，需要进行 K 次交换操作，每次需要 3 个赋值语句，所以交换操作总耗时 3 * K 个单位时间。而插入排序中移动数据的总时间为 K 个单位时间。

**特定的算法是依赖特定的数据结构的。上面几种算法都是基于数组的。如果将数据结构存储在链表中，这三种排序还能正常工作吗？如果能，那相应的时间、空间复杂度是多少？**

> ​	对于这个问题，首先要明确一个前提，是否允许修改链表节点的 value 值，还是只能改变节点的位置。一般而言，考虑只能改变节点位置的情况。冒泡排序相比于数组实现，比较次数一致，但是交换操作更加复杂了。插入排序相比于数组实现，比较次数一致，但是插入操作不需要再改变后续节点，但如果是单链表，排序结束后可能要倒置链表。而对于选择排序，比较次数和插入次数都一致，但是由于链表的插入操作更复杂一点，可能会比数组操作慢一点点，变化不大。
>
> 综上，冒泡排序会慢一些，而选择排序系数会减小，选择排序变化不大。

## 3.归并排序

​	归并排序和快排都用到了分冶思想，可以借助这个思想解决非排序问题。比如：如何在**O(n)**的时间复杂度内查找一个无序数组中的第**K**大元素? 

### 3.1 原理(先分后合)

​	归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。

​	分治算法一般都是用递归来实现的。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。

> 递推公式: merge_sort(p...r) = merge(merge_sort(p...q), merge_sort(q+1...r)) 
>
> 终止条件:p >= r 不用再继续分解 
>
> merge_sort(p...r)表示，给下标从p到r之间的数组排序。我们将这个排序问题转化为了两个子问题，merge_sort(p...q)和merge_sort(q+1...r)，其中下标q等于p和r的中间位置，也就是(p+r)/2。当下标从p到q和从q+1到r这两个子数组都排好序之后，我们再将两个有序的子数组合并在一起，这样下标从p到r之间的数据就也排好序了。 
>
> 即：取数组中间位置，将数组拆分为两个小数组，然后分别进行排序，最后再进行合并。

伪代码：

```Java
// 归并排序算法, A是数组，n表示数组大小 merge_sort(A, n) {
}merge_sort_c(A, 0, n-1)
// 递归调用函数 merge_sort_c(A, p, r) {
// 递归终止条件
if p >= r then return
// 取p到r之间的中间位置q q = (p+r) / 2
// 分治递归 merge_sort_c(A, p, q) merge_sort_c(A, q+1, r)
// 将A[p...q]和A[q+1...r]合并为A[p...r] }merge
```

​	**merge(A[p...r], A[p...q], A[q+1...r])这个函数的作用就是，将已经有序的A[p...q]和A[q+1...r]合并成一个有序的数组，并且放入A[p...r]。**

如图所示，我们申请一个临时数组tmp，大小与A[p...r]相同。我们用两个游标i和j，分别指向A[p...q]和A[q+1...r]的第一个元素。比较这两个元素A[i]和A[j]，如 果A[i]<=A[j]，我们就把A[i]放入到临时数组tmp，并且i后移一位，否则将A[j]放入到数组tmp，j后移一位。 

继续上述比较过程，直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的 就是两个子数组合并之后的结果了。最后再把临时数组tmp中的数据拷贝到原数组A[p...r]中。 

![image-20190415082224553](/Users/jack/Desktop/md/images/image-20190415082224553.png)

### 3.2 性能分析

归并排序是一个稳定的排序算法。

#### 时间复杂度：

​	因为合并排序是通过递归去解决的，假设一个问题a可以分解为多个子问题b、c，那求解问题a就可以分解为求解问题b、c，求解完b，c两个问题之后才将它们的结果合并。所以我们定义求解问题a的时间是T(a)，求解问题b、c的时间分别是T(b)和 T( c)，那我们就可以得到这样的递推关系式:

> T(a) = T(b) + T(c) + K
>
> 其中K等于将两个子问题b、c的结果合并成问题a的结果所消耗的时间。

​	我们假设对n个元素进行归并排序需要的时间是T(n)，那分解成两个子数组排序的时间都是T(n/2)。我们知道，**merge()函数合并两个有序子数组的时间复杂度是O(n)。**所以，套用前面的公式，归并排序的时间复杂度的计算公式就是: T(1) = C; n=1时，只需要常量级的执行时间，所以表示为C。 

> T(n) = 2*T(n/2) + n; n>1 

通过这个公式，如何来求解T(n)呢?

> T(n) = 2*T(n/2) + n
>  = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n
>  = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n
>  = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n .
>
> .....
>  = 2^k * T(n/2^k) + k * n 

​	通过这样一步一步分解推导，我们可以得到T(n) = 2^k*T(n/2^k)+k*n。当T(n/2^k)=T(1)时，也就是n/2^k=1，我们得到k=log2n 。我们将k值代入上面的公式，得到T(n)=C*n+n*log2n 。如果我们用大O标记法来表示的话，T(n)就等于O(nlogn)。所以==归并排序的时间复杂度是O(nlogn)。== 

​	**从我们的原理分析和伪代码可以看出，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是O(nlogn)。** 

#### 空间复杂度：

​	**归并排序不是原地排序算法。**这是因为归并排序的合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间。

> ​	实际上，递归代码的空间复杂度并不能像时间复杂度那样累加。刚刚我们忘记了最重要的一点，那就是，**尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。**==在任意时刻，CPU只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过n个数据的大小，所以空间复杂度是O(n)。==

```Java
public static void mergeSort(int[] arr) {
        if (arr == null || arr.length < 2) {
            return;
        }
        mergeSort(arr, 0, arr.length - 1);
    }

    /**
     * @param arr   待排序的数组
     * @param l     数组的左边界
     * @param r     数组的右边界
     * 归并排序，递归排序后调用merge方法将两个数组合并
     */
    public static void mergeSort(int[] arr, int l, int r) {
        if (l == r) {
            return;
        }
        // 避免数值溢出，所以用这种方式计算中间的数，>>表示除以2的1次方
        int mid = l + ((r - l) >> 1);
        mergeSort(arr, l, mid);
        mergeSort(arr, mid + 1, r);
        merge(arr,l,mid,r);
    }

    /**
     * @param arr       待merge的数组
     * @param l         数组的左边界
     * @param m         数组的中间数
     * @param r         数组的右边界
     */
    public static void merge(int[] arr, int l, int m, int r) {
//        创建一个跟arr一样长度的数组，作为存储arr的值,help是一个辅助数组
        int[] help = new int[r - l + 1];
        int i = 0;
//        p1,p2分别是两个分组中最小的值，即分别指向排序好后的两个数组的第一位
        int p1 = l;
        int p2 = m + 1;
//        对help进行赋值和排序，分为前后两部分赋值,哪边小填哪个，就一个排序的过程
        while (p1 <= m && p2 <= r) {
            // 每次存入之后，对应的数组索引就加一
            help[i++] = arr[p1] < arr[p2] ? arr[p1++] : arr[p2++];
        }
//        如果上面的循环跳出之后，那么p1,p2必有一个越界，需要存入还没越界的值
// 下面两个while循环不会同时运行，如果p1越界，则p2不越界，存入p2的值
        while (p1 <= m) {
            help[i++] = arr[p1++];
        }
        while (p2 <= r) {
            help[i++] = arr[p2++];
        }
        for (i = 0; i < help.length; i++) {
            arr[l + i] = help[i];
        }
    }
```

## 4.快速排序

### 4.1 原理分析

​	**快排的思想是这样的:如果要排序数组中下标从p到r之间的一组数据，我们选择p到r之间的任意一个数据作为pivot(分区点)。**

> ​	我们遍历p到r之间的数据，将小于pivot的放到左边，将大于pivot的放到右边，将pivot放到中间。经过这一步骤之后，数组p到r之间的数据就被分成了三个部分，前面p到q-1之间都是小于pivot的，中间是pivot，后面的q+1到r之间是大于pivot的。

![image-20190415084954962](/Users/jack/Desktop/md/images/image-20190415084954962.png)

主要在于partition()分区函数，采取原地分区的做法：

> partition(A, p, r) {
>
> ​	 pivot := A[r]
> ​	 i := p
> ​	 for j := p to r-1 do { 
>
> ​		if A[j] < pivot { 
>
> ​			swap A[i] with A[j] 
>
> ​	}
>
> }
>
> i := i+1 
>
> swap A[i] with A[r] return i 
>
> ​	我们通过游标i把A[p...r-1]分成两部分。A[p...i-1]的元素都是小于pivot的，我们暂且叫它“已处理区间”，A[i...r-1]是“未处理区间”。我们每次都从未处理的区间A[i...r-1]中取一个元素A[j]，与pivot对比，如果小于pivot，则将其加入到已处理区间的尾部，也就是A[i]的位置。

### 4.2 性能分析

​	T(n)在大部分情况下的时间复杂度都可以做到O(nlogn)，只有在极端情况下，才会退化到O(n2)。

### 4.3 解决Kth问题

​	快排核心思想就是分治和分区，我们可以利用分区的思想，来解答Kth问题:O(n)时间复杂度内求无序数组中的第K大元素。

​	比如，4， 2， 5， 12， 3这样一组数据，第3大元素就是4。 我们选择数组区间A[0...n-1]的最后一个元素A[n-1]作为pivot，对数组A[0...n-1]原地分区，这样数组就分成了三部分，A[0...p-1]、A[p]、A[p+1...n-1]。 

​	如果p+1=K，那A[p]就是要求解的元素;如果K>p+1, 说明第K大元素出现在A[p+1...n-1]区间，我们再按照上面的思路递归地在A[p+1...n-1]这个区间内查找。同 理，如果K<p+1，那我们就在A[0...p-1]区间查找。 

![image-20190225153911990](/Users/jack/Desktop/md/images/image-20190225153911990.png)

# 八、线性排序

## 桶排序

桶排序，顾名思义，会用到“桶”。核心思想是将要排序的数据分到几个有序的桶里，再对每个桶里的数据进行单独排序。最后再把每个桶里的数据按顺序连接起来，组成的序列就是有序的了。

-  **对数据的要求**： 待排数据要能很容易划分成 n 个桶，桶之间有天然的大小顺序。并且数组在各个桶之间的分布是比较均匀的。
-  **时间空间复杂度**： 假设 n 个数据均匀地分布到 m 个桶内，每个桶就有 k = n/m 个元素。对桶内每个数据使用快排，总时间复杂度 O(m * k * logk) = O(n * log(n/m))。当 m 接近 n 时，时间复杂度为 O(n)。当数据分布很不均匀时，时间复杂度会退化到 O(nlogn)。
-  **应用场景**： 可以用于外部排序。

> 比如用几百 M 内存排序 10GB 的订单数据（假设金额都是正整数）。
>
> 我们可以先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是 1 元，最大是 10 万元。我们将所有订单根据金额划分到 100 个桶里，第一个桶我们存储金额在 1 到 1000 元之间的订单，第二桶存储金额在 1001 元到 2000 元之间的订单，以此类推。每一个桶对应一个文件，并且按照金额大小顺序命名（00，01，02…99）。
>
> 在理想情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的数据，我们就可以将这 100 个小文件依次放到内存中，用快排来排序。等所有文件都排好序之后，我们只要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小打到排序的订单数据了。
>
> 不过，订单金额很可能是分布极不均匀的，若是集中在某个区间，比如 1 到 1000 元中间比较多，那我们还可以对这个区间再进行具体的划分，比如 1 到 100 是一个区间，101 到 200 是第二个区间，以此类推。如果划分之后，某个区间的订单仍然太多，那还可以继续划分，直到所有文件都可以读入内存为止。

## 计数排序

计数排序应该是桶排序的一种比较特殊的情况。这种情况要求待排序的数据集中在一个不大的范围内，比如最大值是 K ，我们就可以把数据分成 K 个桶，每个桶里的数据都是相同的，省掉了桶内排序的时间。若有负数，还需要设置偏移量。

-  **对数据的要求**： 数据集中在一个较小的范围内。
-  **时间空间复杂度**： 时间复杂度是 O(n)。只有只有三趟遍历，第一趟计数，第二趟映射到辅助数组，第三趟拷贝回原数组。空间复杂度是 O(n)，虽然省掉了桶内排序的时间，但是空间没有减少。
-  **应用场景**： 比如对高考考生成绩排序，只有几百种情况，直接开几百个桶就可以了。

## 基数排序

是一种多关键字排序方法。从关键字的后部往前进行多轮 O(n) 的稳定性排序排序。

-  **对数据的要求**： 待排数据的键值部分可以分成几个部分。
-  **时间空间复杂度**： 总时间复杂度是键值部分个数 m 的 O(n) 倍，由于键值部分个数一般是一个很小的常量，所以，总时间仍是 O(n)。空间复杂度是 O(n)。
-  **应用场景**： 对手机号进行排序。键值部分个数是 11 个，总共进行 11 趟。其他情况若是关键字部分长度不一，可以采用填充法。

## 课后思考

根据年龄给 100 万用户排序

使用桶排序，开 120 个桶，看情况要不要进行外部排序。

对包含大写字母、小写字母、数字的一个字符串按照小写字母在前，大写字母在最后，数字在中间，不用排序算法进行排列。

这是一个荷兰国旗问题。

使用三个指针，pre, current, last 分别指向小写字母部分的结尾，数字部分的开头，大写字母部分的开头，采用交换元素的方法，遍历一遍待排字符串即可。时间复杂度 O(n)，空间复杂度 O(1)。

# 九、优化快速排序

![image-20190428110501289](/Users/jack/Desktop/md/images/image-20190428110501289.png)

​	如果对小规模数据进行排序，可以选择时间复杂度是O(n2)的算法;如果对大规模数据进行排序，时间复杂度是O(nlogn)的算法更加高效。

> Java语言采用堆排序实现排序函数，C语言使用快速排序实现排序函数。
>
> 快排最坏情况的O(n2)主要原因还是==因为分区点选的不够合理。==所以，为了提高排序算法的性能，要尽可能地让每次分区都比较平均。

- 三数取中法，九数取中法
- 随机法

​	快速排序是用递归来实现的，递归要警惕堆栈溢出。为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，有两种解决办法:

- 限制递归深度
- 自己实现函数调用栈，手动模拟入栈出栈

## Glibc 中的 qsort()

- 小规模排序使用归并排序，空间换时间。
-  数据规模大时，使用快排。
-  快排采用三数取中法。
-  并采用自己实现堆上的栈，手动模拟递归来解决。
-  在快排中，当元素规模小于 4 时，不再递归，使用插入排序。
-  插入排序使用哨兵优化。

### Java 中的基本元素排序

对基本元素类型，使用双轴快排

### Java 中的引用元素排序

在 jdk 8 中，保留一个参数，若是开启，会使用归并排序。
 默认使用 TimSort。大致思路如下：

1. 元素个数 < 32，采用二分查找插入排序(Binary Sort)。
2. 元素个数 >= 32，采用归并排序，归并的核心是分区(Run)。
3. 找出连续升或降的序列作为分区，分区最终倍调整为升序后压入栈。
4. 如果连续分区长度太小，通过二分插入排序扩充长度到分区最小阈值。
5. 每次压入栈，都要检查已存在的分区是否满足合并条件，满足则进行合并。
6. 最终栈内的分区倍全部合并，得到一个排序好的数组。

TimSort 的算法非常巧妙：

1. 找出左分区最后一个元素（最大）及在右分区的位置
2. 找出右分区第一个元素（最小）及在左分区的位置
3. 仅针对这两个位置进行合并，之外的元素元素本身就是有序的。

# 十、二分搜索

​	二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为0。时间复杂度是O(logn)

实现代码：

```java
/**
     * @param a     待查找数组
     * @param n     数组长度
     * @param value 待查找的值
     * @return
     */
    public int bSearch(int[] a, int n, int value) {
        int low = 0;
        int high = n - 1;
        while (low <= high) {
            int mid = low + (high - low) / 2;
            if (a[mid] == value) {
                return mid;
            } else if (a[mid] < value) {
                low = mid + 1;
            } else {
                high = mid - 1;
            }
        }
        return -1;
    }
```

使用场景：

- 数据是通过顺序表结构存储的，比如数组；
- 有序数据，若无序需要先排序
- 数据量需要比较大

## 变形的二分查找

上面的二分查找是最简单的一种，即有序数据集合中不存在重复的数据，我们在其中查找值等于某个给定值的数据。

![image-20190502120336310](/Users/jack/Desktop/md/images/image-20190502120336310.png)

1.在有序重复数组中，查找第一个值等于给定值的元素

```java
public int bSearch(int[] a, int n, int value) {
        int low = 0;
        int high = n - 1;
        // 注意循环退出的条件是low <= high，而不是low < high，因为等于也成立
        while (low <= high) {
//            int mid = low + (high - low) / 2;
            int mid = low + ((high - low) >> 1);
            if (a[mid] > value) {
                high=mid-1;
            } else if (a[mid] < value) {
                low = mid + 1;
            } else {    //a[mid]==value的时
//如果此时mid是第一个元素的话，因为是排序数组，所以肯定是第一个相等的
//如果mid的前一个元素不等于value，那么表明mid也是第一个相等的，所以这两种情况都是返回mid
//如果上面的情况都不符合，那么表明不是第一个相等的，所以将high左移,第一个在左边区域，继续遍历                
                if ((mid==0)||a[mid-1]!=value) return mid;
                else high=mid-1;
            }
        }
        return -1;
    }
```

2.在有序重复数组中，查找最后一个值等于给定值的元素

对上面的else进行修改即可。

```Java
 else {    //a[mid]==value的时
//如果此时mid是最后一个元素的话，因为是排序数组，所以肯定是最后一个相等的
//如果mid的后一个元素不等于value，那么表明mid是最后一个相等的，所以这两种情况都是返回mid
//如果上面的情况都不符合，那么表明不是最后一个相等的，所以将low右移,最后一个相等的在右边区域，继续遍历                
                if ((mid==n-1)||a[mid+1]!=value) return mid;
                else low=mid+1;
            }
```

3.在有序重复数组中，查找第一个大于等于给定值的元素。

思路与上面的两种变形类似。

```Java
public int bSearch(int[] a, int n, int value) {
        int low = 0;
        int high = n - 1;
        // 注意循环退出的条件是low <= high，而不是low < high，因为等于也成立
        while (low <= high) {
//            int mid = low + (high - low) / 2;
            int mid = low + ((high - low) >> 1);
            if (a[mid] >= value) {  // 查看是否为第一个大于value的数
                if ((mid == 0) || a[mid - 1] < value) return mid;
                else high = mid - 1;
            } else {
// 如果a[mid]小于要查找的值value，那要查找的值肯定在[mid+1, high]之间，所以low=mid+1。
                low = mid + 1;
            }
        }
        return -1;
    }
```

4.在有序重复数组中，查找最后一个小于等于给定的元素。

```Java
public int bSearch(int[] a, int n, int value) {
        int low = 0;
        int high = n - 1;
        // 注意循环退出的条件是low <= high，而不是low < high，因为等于也成立
        while (low <= high) {
//            int mid = low + (high - low) / 2;
            int mid = low + ((high - low) >> 1);
            if (a[mid] > value) {
                high=mid-1;
            } else {
                // 查看是否为最后一个小于等于value的数
                if ((mid == n-1) || a[mid +1] > value) return mid;
                else low = mid +1;
            }
        }
        return -1;
    }
```

# 十一、跳表

​	==跳表是通过对链表建立多级索引实现的一种可以在插入、删除、查找、取连续区间数据性能极其卓越的动态数据结构。时间复杂度都是 O(logn)。==Redis中的有序集合(Sorted Set)就是用跳表和双HashMap构成的字典来实现的。

## 理解跳表

![image-20190502235645162](/Users/jack/Desktop/md/images/image-20190502235645162.png)

​	如果对一个长度为 n 的链表，每两个元素取第一个元素建立索引，那么查找元素的时间就可以降低 1/2，再对这层索引，仍每两个元素取第一个元素建立索引，在这一层查找元素的时间就会降低为下一层索引的 1/2，即这一层总的查找时间为原来的 1/2 * 1/2 = 1/4……最终顶层为两个索引元素，整个数据结构的查找时间就降低为 O(logn)，这就是跳表。

或者抽取多个节点提到上层：

![image-20190502235900994](/Users/jack/Desktop/md/images/image-20190502235900994.png)

## 跳表到底有多快

​	假设从底到顶每层索引标记为第 1 层索引，第 2 层索引……每层节点的数量是 n / (2^k)。若顶层有两个节点，那么总共有 log[2]n-1 层索引，包含底层链表，就是 log[2]n 层，又每层需要遍历不超过 3 个节点，总共的查找时间就是 O(3 * logn) = O(logn)。

从这个角度看，我们通过空间换时间，用链表实现了 “二分查找”。

**如果我们在链表中存储的是大对象，我们在索引中只是保存要比较的 key。那这点空间又其实可以忽略。**

## 维护跳表索引

![image-20190503000031692](/Users/jack/Desktop/md/images/image-20190503000031692.png)

​	跳表如果插入元素时索引没有即使地动态建立，那就可能造成索引和原始链表地大小不平衡，导致复杂度退化到链表级别。

​	当我们往跳表中插入数据的时候，我们可以根据随机函数选择将这个数据插入到部分索引层中。比如随机函数生成了值 K，那我们就将这个结点添加到第 1 级到第 k 级的索引之中。

随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。

在跳表中删除元素时，除了要删除链表中的，还要删除索引中的，这就要拿到并保持前驱节点，或者用双向链表。

## 跳表在 redis 中的实现

Redis 中的有序集合是通过跳表和散列表来实现的。有序集合的功能大概有以下几点：

- 插入一个元素
- 查找一个元素-
- 删除一个元素
- 按照区间查找数据
- 迭代输出有序序列

其中，插入、删除、查找、迭代输出有序序列这几个操作，红黑树也可以完成，复杂度一样。但是按照区间查找数据这个功能，红黑树实现的效率就没有跳表高了。

跳表只需要在 O(logn) 时间内定位到区间的开头，然后往后遍历就可以了。

# 十二、散列表

## 散列思想

​	散列表利用的是数组支持下标随机访问，时间复杂度是 O(1) 的特性，**==通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。==**所以散列表其实是数组的一种扩展，由数组演化而来。

​	散列思想就是，通过散列函数，把元素的键值 (key) 映射为数组的下标 (hash(key))，然后将元素存储在数组中对应下标的位置。当我们按照键值 (key) 查找元素时，再利用同样的散列函数，将键值映射为数组下标，从对应下标的位置取数据。

## 散列函数

​	散列函数，顾名思义，它是一个函数，符合 y = f(x) 的特征。我们可以把散列函数定义为 hash(key) , 其中 key 表示元素的键值，hash(key) 表示经过散列函数计算得到的散列值。

散列函数要符合三个基本设计要求：

1. 散列函数集散得到的散列值是一个非负整数;因为数组下标是从 0 开始的。
2. 如果 key1 = key2，那 hash(key1) == hash(key2);相同的 key，经过散列函数映射得到的散列值也应该是相同的，这样我们才能取回存放的元素。
3. 如果 key1 != key2，那 hash(key1) != hash(key2)。这一点是理想情况，但现实要想找到一个不同 key 对应的散列值都不一样的散列函数，几乎是不可能的。因为数组存储的空间有限，而 key 可以无限，所以总有重复的时候，即总会存在散列冲突。

## 散列冲突

**解决散列冲突的方法很多，常用的有两类：开放寻址法和链表法。** Java 中的 LinkedHashMap 就采用了链表法解决冲突，ThreadLocalMap 是通过线性探测的开放寻址法来解决冲突。

### 开放寻址法

​	开放寻址法的核心思想就是，如果出现了散列冲突，我们就再重新探测一个空闲位置，将其插入，比如在hash函数计算出来的位置如果非空置，那么就继续往下遍历寻找。当删除元素时，将该位置的元素标记为 deleted 而不是直接删除。

​	开放寻址法比较适合小数据量，当存在冲突时，速度比链表法快一些，毕竟可以直接利用 CPU 的缓存把整个数组缓存下来， IO 时间快了很多。Java 中的 ThreadLocalMap 就用了开放寻址法。

> 这种实现方式，散列表中的数据都存放在数组中，可以有效利用 CPU 的缓存加快查询速度。序列化过程比较简单。链表法包含指针，序列化就比较麻烦。
>
> 缺点是，删除数据比较麻烦，需要使用特殊标记。所有数据都存在一个数组中，冲突的代价很大。
>
> ==总结，数据量小、装载因子小，适合开放寻址法。这也是 ThreadLocalMap 采用开放寻址法解决散列冲突的原因==。

重新探测新的位置的方法有很多，比如：

- 线性探测
- 二次探测 （二次方探测）
- 双重散列（使用一组散列函数，一个不行换另一个）

### 链表法

​	链表法比开放寻址法更加常用，散列冲突时，将冲突的元素组织成一条链表、或者变种成一棵树。删除元素时可以直接删除。

​	链表法的优点是内存利用率高。链表节点可以在需要的时候再创建，并不需要像开放寻址法那样实现申请好。

​	链表法对冲突的容忍性更高，装载因子可以大于 1，甚至再散列冲突很平均的情况下，即使装载因子达到 10，查找效率也不会大幅度衰退。更进一步，对链表法进行改造，使用红黑树或者跳表解决散列冲突，那即使是极端情况下，所有数据都存放在一个槽内，查询时间也是衰退到 O(logn) 的数量级。

​	缺点是，需要维持链表的指针引用，若是存放小对象，那么指针占用的内存就会对比起来挺多，而且链表法也容易造成内存碎片。

​	==总结起来就是，基于链表法的散列冲突方法适合存储大对象、大数据量的散列表，而且比起开放寻址法，它更加灵活，支持更多的优化策略。==

## 散列表应用

1.实现一个 Word 文档中档次的拼写检查功能

​	常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节，那 20 万个单词大概占用 2MB 的存储空间，就算放大十倍也就是 20MB，对于现在的计算机来说，这个大小完全可以放在内存里面。所以可以利用散列表来储存整个英文单词词典。

​	当用户输入某个英文单词时，我们就拿这个单词取散列表中查找，如果查到，就说明是拼写检查正确的；否者，就提示可能存在拼写错误。

2.假设我们有 10 万条 URL 访问日志，如何按照访问次数给 URL 排序？

​	遍历 10 万条数据，URL 作为 key，value 是访问次数，存入散列表，同时记录下最大的访问次数 K，时间复杂度是 O(n)。==如果 K 不是很大，可以使用桶排序，时间复杂度 O(N)。如果 K 非常大(比如大于 10 万)，就使用快速排序，复杂度 O(NlogN)。==。

3.有两个字符串数组，每个数组大约有 10 万条字符串，如何快速找出两个数组中相同的字符串？

1. 以第一个数组为基础构建一个散列表，字符串作为 key，value 是一个标记。遍历第二个数组，每个字符串作为 key 往散列表中做查询，查不到的字符串存到一个 set 中，遍历结束，这个 set 中就存放着两个数组中相同的字符串。
2. 直接使用 hashset ，若是 add 失败，就是重复的。

散列函数的设计的好坏，决定了散列冲突的概率大小，也直接决定了散列表的性能。

### 好的散列函数的特点

- 散列函数的设计不能太过复杂。

过于复杂的散列函数，会消耗过多的计算时间，也就间接影响到散列表的性能。

- 散列函数生成的值应该尽可能随机且均匀分布

这样才能避免或者最小化散列冲突。即便是出现冲突，散列到每个槽的数据也会分布得比较均匀，不会出现某个槽内的数据特别多的情况。

- 实际工作中还需要综合考虑各种因素。

这些因素有关键字的长度、特点、分布、还有散列表的大小等。

### 常用的散列函数设计方法

- 数据分析法

​        通过分析传入散列函数的 key 的特征规律，设计相应的专用散列函数。比如处理使用散列函数处理手机号码，考虑到手机号码前几位重复性很大，后几位比较随机，我们就可以取手机号码后几位作为散列值。

- 进位相加

​        比如对字符串进行散列，可以将字符串中每个字母的 ASCII 值进行“进位”相加，然后再跟散列表的大小求余、取模，作为散列值。比如，对因为单词 nice 进行散列：
 `hash("nice")=(("n" - "a") * 26*26*26 + ("i" - "a")*26*26 + ("c" - "a")*26+ ("e"-"a")) / 78978`

- 直接寻址法、平方取中法、折叠法、随机数法等

## 装载因子过大了怎么办

​	==装载因子的定义为 `散列表的装载因子 = 填入表中的元素个数 / 散列表的长度`，==根据这个公式，可以推出，装载因子越大且散列表的长度不变，则散列表中的元素越多，空闲位置越少，散列冲突的概率会变大。不仅插入数据的过程要多次寻址或者拉很长的链，查找的过程也会因此变得很慢。

​	对没有频繁插入和删除的静态数据集合来说，我们很容易根据数据的特点、分布等，设计出完美的、极少冲突的散列函数。

对于动态散列表来说，数据集合频繁变动，负载因子可能随着时间不断变大。这时就要动态扩容。

动态扩容的过程，简单来说，就是申请一个新的散列表，然后把原来的数据搬运到新的散列表中，但是不是简单的搬运，而是每个元素都要根据新的散列表重新存储位置。

这个过程，运用平摊分析法，每个插入操作的时间复杂度仍是 O(1)。

扩展一下，当散列表的装载因子小于某个阈值时，我们也可以进行动态缩容。

## 如何避免低效扩容

​	大部分情况下，动态扩容的散列表插入一个数据都很快，但在特殊情况下，装载因子达到扩容的阈值，此时，再插入数据，就会触发漫长的扩容过程，在特定的场合，这样漫长的等待过程是不可接受的。

​	举个直观的例子，假设原来散列表有 1GB 的数据，现在进行扩容，就需要对这 1GB 的数据进行再散列，这个扩容的时间，看起来就很耗时。

​	在这种情况下，“一次性”扩容的机制就不适合了。此时，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子达到扩容的阈值后，我们只申请新空间，但不立即将老的数据全部搬移到新的散列表中。

![image-20190503224322391](/Users/jack/Desktop/md/images/image-20190503224322391.png)

​	当有新的数据插入时，我们就将新数据插入新散列表中，同时从旧散列表中拿出一个数据放入到新的散列表。每次插入一个数据到散列表，我们都重复以上的过程。经过多次操作后，老的散列表中的数据就一点一点全部搬移到新的散列表中了。没有了一次性全部搬移，插入操作就不会出现一次很慢的情况了。

​	**这期间的插入操作，可以先从新的散列表中查，查不到再去旧的散列表中查。甚至，可以在每次查询操作中也穿插一条数据搬移。**

## 工业级散列表HashMap

1.初始大小

HashMap 的初始大小是 16，当然这个默认值是可以设置的，如果事先知道大概的数据量多少，就可以依此设置合适的初始容量。

2.装载因子和动态扩容

​	最大装载因子默认是 0.75。当 HashMap 中元素个数超过 0.75 * capacity 时，就会启动扩容，每次扩容后的容量是原来的两倍。

关于 0.75 这个数字的由来，可以查看这篇文章 <https://blog.csdn.net/reliveIT/article/details/82960063>。

3.散列冲突解决办法

​	HashMap 底层采用链表法解决冲突。在 JDK 1.8 之前，冲突的元素插入链表首端，在 JDK 1.8 之后，插入尾端。另外，在 JDK 1.8 之后，当链表长度超过 8 时，会启动树化，当树中元素少于 6 个时，会退化回链表。

4.散列函数

HashMap 的二次哈希，使用的是除留余数法。

因为 A % B = A & (B - 1)，所以，(h ^ (h >>> 16)) & (capitity -1) = (h ^ (h >>> 16)) % capitity。

## 工业级散列表应该具有的特征

- 支持快速的查询、插入、删除操作
- 内存占用合理，不能浪费过多内存
- 性能稳定，极端情况下也不会性能退化到无法接受

设计这样的散列表应该从三个方面考虑

- 设计合适的散列函数
- 定义合适的装载因子
- 合适的动态扩容策略
- 合适的散列冲突解决办法

## 为什么散列表和链表经常会一起使用？

​	因为散列表有 O(1) 的时间查找、删除数据的特性，但是元素是无序的。而链表中的数据可以是有序的，可以顺着指针按顺序遍历所有节点，但是在链表中查找数据的时间是 O(n)。

很明显，通过时间换空间，我们同时对一组数据建立链表和散列表两种数据结构，并且组合在一起，就可以构造出一种兼具两者优点的数据结构——快速的插入、查找、删除和按顺序遍历功能。

这个组合的数据结构示意图如下：



同样的道理，如果我们组合的散列表 + 跳表，我们就可以在上面的基础上额外获得在 O(logn) 的时间内定位元素区间的数据的功能。

## 课后思考

1. 今天讲的几个散列表和链表组合使用的例子里，我们用的都是双向链表，如果把双向链表改成单链表，还能否正常工作？为什么呢？

- 对于 LRU 缓存淘汰算法，可以改装成单链表

在这个算法里，我们使用链表主要是为了维护有序性（按照元素访问的顺序排列），并不需要逆序输出序列，此时对链表的要求只是删除一个节点，然后把这个节点插入到链表的尾端。我们可以通过特殊的技巧，把下一个节点的数据移到这个节点，然后下个节点来达到类似删除本节点的效果。这个删除时间同样是 O(1)，除了删除的节点是尾节点，就需要 O(n) 的时间找到它的上一个节点。

- 对于需要逆序输出一个区间的数据的情况，最好使用双链表

1. 假设猎聘网有 10 万名猎头，每个猎头都可以通过做任务（比如发布职位）来积累积分，然后通过积分来下载简历。假设你是猎聘网的一名工程师，如何在内存中存储这 10 万个猎头 ID 和积分信息，让它能够支持这样几个操作：

- 根据猎头的 ID 快速查找、删除、更新这个猎头的积分信息；
- 查找积分在某个区间的猎头 ID 列表；
- 查找按照积分从小到大排名在第 x 位到第 y 位之间的猎头 ID 列表。

对于这个问题，我们可以维护一个散列表 + 跳表的结构。

散列表的 key 存放猎头的 ID，跳表的索引存放猎头的积分，每次猎头的积分发生变化，就从跳表中删除这个节点，再在合适的位置重新插入这个节点，达到维持底层链表有序性的效果。

这样实现起来，按照 ID 查找、删除猎头的积分信息的效率是 O(1)，更新积分的效率是 O(logn)，查找积分区间的猎头的时间是 O(logn)。但是最后一个操作暂时还不知道怎么实现。































参照：https://www.jianshu.com/p/f1eaf1e8e073

《数据结构与算法之美》、<https://www.jianshu.com/nb/29712522>
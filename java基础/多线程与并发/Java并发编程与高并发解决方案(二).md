# Java并发编程与高并发解决方案(二)

# 9.线程死锁

## 1、死锁概念

​	通俗的说，死锁就是两个或者多个线程，相互占用对方需要的资源，而都不进行释放，导致彼此之间都相互等待对方释放资源，产生了无限制等待的现象。死锁一旦发生，如果没有外力介入，这种等待将永远存在，从而对程序产生严重影响。 

## 2.1、死锁产生的必要条件

- 互斥条件：进程对所分配到的资源不允许其他进程进行访问，若其他进程访问该资源，只能等待，直至占有该资源的进程使用完成后释放该资源
- 请求和保持条件：进程获得一定的资源之后，又对其他资源发出请求，但是该资源可能被其他进程占有，此时请求阻塞，但又对自己获得的资源保持不放
- 不剥夺条件：是指进程已获得的资源，在未完成使用之前，不可被剥夺，只能在使用完后自己释放
- 环路等待条件：是指进程发生死锁后，必然存在一个进程–资源之间的环形链

例子：

```java
@Slf4j
public class DeadLock implements Runnable {
    public int flag = 1;
    //静态对象是类的所有对象共享的
    private static Object o1 = new Object(), o2 = new Object();
    @Override
    public void run() {
        log.info("flag:{}", flag);
        if (flag == 1) {
            synchronized (o1) {
                try {
                    Thread.sleep(500);
                } catch (Exception e) {
                    e.printStackTrace();
                }
                synchronized (o2) {
                    log.info("1");
                }
            }
        }
        if (flag == 0) {
            synchronized (o2) {
                try {
                    Thread.sleep(500);
                } catch (Exception e) {
                    e.printStackTrace();
                }
                synchronized (o1) {
                    log.info("0");
                }
            }
        }
    }

    public static void main(String[] args) {
        DeadLock td1 = new DeadLock();
        DeadLock td2 = new DeadLock();
        td1.flag = 1;
        td2.flag = 0;
        //td1,td2都处于可执行状态，但JVM线程调度先执行哪个线程是不确定的。
        //td2的run()可能在td1的run()之前运行
        new Thread(td1).start();
        new Thread(td2).start();
    }
}
```

上述代码出现死锁原因：

​	当DeadLock类的对象flag\==1时（td1），先锁定o1,睡眠500毫秒，而td1在睡眠的时候另一个flag==0的对象（td2）线程启动，先锁定o2,睡眠500毫秒。td1睡眠结束后需要锁定o2才能继续执行，而此时o2已被td2锁定；
td2睡眠结束后需要锁定o1才能继续执行，而此时o1已被td1锁定；td1、td2相互等待，都需要得到对方锁定的资源才能继续执行，从而死锁。

## 2.2、处理死锁的基本方法

1.预防死锁：通过设置一些限制条件，去破坏产生死锁的必要条件

2.避免死锁：在资源分配过程中，使用某种方法避免系统进入不安全的状态，从而避免发生死锁(如**银行家算法**)

3.检测死锁：允许死锁的发生，但是通过系统的检测之后，采取一些措施，将死锁清除掉

4.解除死锁：该方法与检测死锁配合使用

## 3、确认死锁

​	在真实的环境中，我们发现程序无法执行，并且CPU占用为0，这样就有理由怀疑产生了死锁，但是光怀疑是不行的，我们需要一个实际的验证方法。接下来我们使用jdk提供的工具来检测是否真正发生了死锁。 
运行上述的代码在终端输入**“jps”**可以看到：

可见控制台输出：我们上边运行的类的类名以及对应的进程ID 
![这里写图片描述](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/70-20190211104302812.png) 

接下来使用命令获取进程对应线程的堆栈信息：

```
jstack 9284 
```

分析堆栈信息（提取有用的部分） 
![这里写图片描述](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/70-20190211104323941.png)

两个线程都进行了加锁操作（如上图） 
![这里写图片描述](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/70-20190211104332901.png) 
系统发现了一个Java-level的线程死锁。ok，确认无疑是发生了死锁现象。

## 4.避免死锁

- 注意加锁顺序（这个很好理解，就像上边的例子）
- 加锁时限（超过时限放弃加锁） 
- 实现方式–使用重入锁。
- 死锁检测（较难，就像分析上边的线程情况）

# 10.高并发

## 10.1 高并发之扩容思路

### 概述

​	 **每个线程都有自己的工作内存**, 占用内存大小取决于工作内存里变量的多少与大小 , 单个线程占用内存通常不会很大, 但是随着并发的线程不断的增加 , 从成百上千, 甚至几十万 , 占用的内存就会越来越多.这时候可能就要考虑给系统扩容了 , 简单点的 升级内存, 复杂点的 , 增加服务器 , 分担压力。

### 扩容方向

1. 垂直扩容：也叫纵向扩容，提高系统部件处理能力(增加内存)；
2. 水平扩容：也叫横向扩容，增加系统内部成员数量(增加服务器)。

#### 数据库扩容

1. 读操作扩展：增加memcache、Redis、CDN等缓存，或使用关系型数据库。

   > ​	假如网站是读操作比较多，比如博客这类。通过通过mysql进行垂直扩展是个不错的选择，并且结合memcache、redis、CDN等构建一个健壮的缓存系统。如果系统超负荷运行，将更多的数据放在缓存中来缓解系统的读压力。采用水平扩容没有太大的意义，因为性能的瓶颈不在写操作，所以不需要实时去完成，用更多的服务器来分担压力性价比太低。所以针对单个系统去强化它的读性能就可以了

2. 写操作扩展：Cassandra、Hbase等

   > ​	假如写操作比较多，比如大型网站的交易系统，可考虑可水平扩展的数据存储方式，比如Cassandra、Hbase等。和大多数的关系型数据库不同，这种数据存储会随着增长增加更多的节点。也可以考虑垂直扩容提升单个数据库的性能，但会发现资金与硬盘的IO能力是有限的，所以需要增加更多数据库来分担写的压力。

## 10.2 高并发之缓存思路

### 1、缓存之特征、场景和组件介绍

#### 概述

​	此处的缓存非彼之计算机缓存。**该缓存主要是降低高并发对数据库冲击，提高数据访问速度、查询命中的数据库缓存。当然也有静态缓存、动态缓存。**

[![图示](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/19.png)](https://suprisemf.github.io/2018/08/03/%E7%BC%93%E5%AD%98%E4%B9%8B%E7%89%B9%E5%BE%81%E3%80%81%E5%9C%BA%E6%99%AF%E5%92%8C%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8Da/19.png)

#### 缓存的特征

1. 命中率（高并发中的重要指标）：**简单表示为：命中数/(命中数+未命中数)**；未命中即未通过缓存获取到想要的数据，可能数据不存在或缓存已过期。

2. 最大元素（或最大空间）：缓存中可以存放的元素数量。一旦缓存中的数据数量超过该值，会触发缓存的清除策略。

3. ==清空策略：FIFO、LFU（使用频率最小策略）、LRU（最近最少被使用策略）、过期时间、随机清除等。==

   > - FIFO
   >   - first in first out，在缓存中最先被创建的数据再被清除时会被优先考虑清除。适用于对数据的实时性要求较高的场景，保证最新的数据可用。
   > - LFU
   >   - 无论数据是否过期，通过比较各数据的命中率，优先清除命中率最低的数据。
   > - LRU
   >   - 无论数据是否过期，通过比较数据最近一次被使用（即调用get()）的时间戳，优先清除距今最久的数据，以保证热点数据的可用性。（多数缓存框架都采用LRU策略）
   > - 过期时间策略
   >   - 通过比较过期时间清除过期时间最长的数据，或通过过期时间，清除最近要过期的数据。

------

#### 缓存命中率的影响因素

1.业务场景和业务需求：显然，**缓存是为了减轻读数据操作的缓冲方式。**

- 适用于读多写少的场景。
- 对数据的实时性要求不高。因为数据缓存的时间越长，数据的命中率越高。

2.缓存的设计：粒度和策略

- 缓存的粒度越小，灵活度高，命中率越高；粒度越小，越不易被其他操作（修改）涉及到，保存在缓存中的时间越久，越易被命中。
- 当缓存中的数据变化时，直接更新缓存中的相应数据（虽然一定程度上会提高系统复杂度），而不是移除或设置数据过期，会提高缓存的命中率。

3.缓存容量和基础设施。

- 当然，缓存容量越大，缓存命中率越高。
- 缓存的技术选型：采用应用内置的内地缓存容易造成单机瓶颈，而采用分布式缓存则具有更好的拓展性，伸缩性。

1. 不同的缓存框架中的缓存命中率也不尽相同。
2. 当缓存结点发生故障，需要避免缓存失效并最大程度地减少影响。可通过一致性hash算法或结点冗余方式避免结点故障。
3. 多线程并发越高，缓存的效益越高，即收益越高，即使缓存时间很短。

------

#### 提高缓存命中率的方法

1. 要求应用尽可能地通过缓存访问数据，避免缓存失效。
2. 结合上面介绍的命中率影响因素：缓存粒度、策略、容量、技术选型等结合考虑。

------

#### 缓存的分类

根据缓存和应用的耦合度进行分类：

#####  1.本地缓存

- 指应用中的缓存组件，主要通过编程实现（使用成员变量、局部变量、静态变量）或使用现成的Guava Cache框架。
- 优点：应用和cache都在同一个进程内部。请求缓存速度快，无网络开销。单机应用中，不需要集群支持时，或集群情况下，各结点不需要互相通知时，适合使用本地缓存。
- 缺点：应用和cache的耦合度高，各应用间无法共享缓存，导致各结点或单机应用需要维护自己的缓存，可能会造成内存浪费。

#####   2.分布式缓存：

- 指的是应用分离的缓存组件或服务，主要现在流行的有：Memcache、Redis。
- 优点：自己本身就是一个独立的服务，与本地应用是隔离的，多个应用可以共享缓存；

#### Guava Cache介绍

它是本地缓存的实现框架。
原理图如下：

[![图示](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/20.png)](https://suprisemf.github.io/2018/08/03/%E7%BC%93%E5%AD%98%E4%B9%8B%E7%89%B9%E5%BE%81%E3%80%81%E5%9C%BA%E6%99%AF%E5%92%8C%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8Da/20.png)

​	可以看出来，它的实现原理类似ConcurrentHashMap，使用多个segment细粒度锁，既保证了线程安全，又支持高并发场景需求。该类Cache类似于一个Map，也是存储键值对的集合，但它还需要处理缓存过期、动态加载等算法逻辑；根据面向对象的思想，它还需要做方法与数据关联性的封装。
​	**Guava Cache实现的主要功能有：自动将结点加入到缓存结构中；当缓存中结点超过设置的最大元素值时，使用LRU算法实现缓存清除；它缓存的key封装在weakReference（弱引用）中，它缓存的value缓存在weakReference（弱引用）或softReference（软引用）中；它可以统计缓存中各数据的命中率、异常率、未命中率等数据；**

#### Memcache

它是一个高效的分布式内存cache，是众多广泛应用的开源分布式缓存的框架之一。

内存结构图如下：

![image-20190211110046097](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211110046097.png)

其中涉及到四个部分：按部分作用区域由大到小分别为

1. slab_class：板层类。
2. slab：板层。
3. page：页。
4. chunk：块。是真正存放数据的地方。

- 同一个slab内的chunk的大小是固定的。而具有相同chunk的slab被分组为chunk_slab。
- Memcache的内存的分配器成为allocator。其中slab的数量有限，与启动参数的配置有关。
- 其中的value总是会被存放到与value占用空间大小最接近的chunk的slab中，以在不降低系统性能情况下节约内存空间。
- 在创建slab时，首先申请内存；其中是以page为单位分配给slab空间，其中，一般page为1M大小；page按照该slab的chunk大小进行切分并形成数组。

Memcache处理原理图如下：

![image-20190211110305436](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211110305436.png)

它本身不提供分布式的解决方案。在服务端，Memcache的集群环境实际上就是一个个Memcache服务器的堆积。它cache的分布式机制是在客户端实现。通过客户端的路由来处理，以达到分布式解决方案的目的。

客户端做路由的原理：

- 客户端采用hash一致性算法，上图中右侧即是其路由的计算方法。
- 相对于一般的hash算法，如取模方式，它除了计算key的hash外，还计算每一个server的hash值，最后将这些hash值映射到一个指定域上，如图中的0-2^32。
- 通过寻找大于key的hash值的最小server作为存储该key的目标server,如果未找到则直接将具有最小hash值的server作为目标server。
- 该机制一定程度上解决了扩容问题。增加或删除某个结点对于该集群来说不会有大影响。
- 最近版本中，Memcache又增加了虚拟结点的设计，进一步提高系统可用性

##### Memcache的内存分配和回收算法

1. 在Memcache的内存分配中，chunk空间并不会被value完全占用，总会有内存浪费；
2. Memcache的LRU回收算法不是针对全局的，而是slab的。
3. Memcache中对允许存放的value占用空间大小有限制。因为内存空间分配是slab以page为单位被分配空间，而page大小规定最大为1M。

##### Memcache的限制和特性

1. Memcache对存储的item数量没有限制；
2. Memcache单进程在32位机器上限制的内存大小为2G，即2的32次方的bit。而64位机器则没有内存大小限制。
3. Memcache的key最大为250个字节；若超过则无法存储。其可存储的value最大为1M，即page最大可分配的大小，此时page中只分配了（形成）一个chunk。
4. Memcache的服务器端是非安全的。当已知一个Memcache结点，外部进入后可通过flushAll命令，使已经存在的键值对立即失效。
5. Memcache不能遍历其中存储的所有item。因为该操作会使其他的访问、创建等操作阻塞，且该进程十分缓慢。
6. Memcache的高性能来源于两个阶段的hash结构：第一个是客户端（该hash算法根据key值算出一个结点）；第二个是服务端（通过一个内部的hash算法，查找真正的item并返回给客户端）。
7. Memcache是一个非阻塞的基于事件的服务程序。

### 2、缓存之初识Redis

#### 概述

Redis即**Re**mote **Di**ctionary **S**ervice的简称，即远程字典服务。
	它是一个远程的非关系型内存数据库。性能强劲。具有复制特性，以及为解决数据而生的独一无二的数据模型：可以存储键值对、以及五种不同类型的值之间的映射。并提供将内存中数据持久化到硬盘功能。可以使用复制特性扩展读性能；可以使用客户端分片扩展写性能。

如下图所示：![image-20190211110451742](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211110451742.png)

#### Redis特点

1. 支持数据的持久化，将内存中数据持久化到硬盘，重启后可以再次加载至内存。
2. 支持上图中string、hash、list、set、sorted set等数据类型。
3. 支持数据的备份，即master-slave主从数据备份。
4. 功能强大：

- 性能极高：读取速度高达11W/s，写速度高达8.1W/s（官方数据）。
- 支持丰富的数据类型。
- 其操作均具有原子性，包括多个操作后另外操作的原子性。
- 支持publish、subscribe、通知、key过期等等功能。

#### Redis适用场景

1. 当需要取出n个最新数据的操作时。
2. 当需要实现排行榜类似的应用时。
3. 当需要精准设定过期时间的应用时。
4. 当需要使用计数器的应用时。
5. 当需要做唯一性检查时。
6. 当需要获取某一时间段内所有数据排头的数据时。
7. 实时系统。
8. 垃圾系统。
9. 使用pub、sub构建消息系统。
10. 构建队列系统。
11. 实现最基础的缓存功能时。

### 3、高并发下缓存的常见问题

#### 缓存一致性

当对缓存中的数据失效要求较高时，需要要求缓存中的数据与数据库中的数据保持一致，其中包括缓存结点与其副本中的数据保持一致。
==缓存的一致性依赖于缓存的过期和更新策略。==

如下图所示：

![image-20190211111400220](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211111400220.png)

导致缓存一致性出现问题的情况：

1. 更新数据库成功，但更新缓存时失败。
2. 更新缓存成功，但更新数据库时失败。
3. 当更新数据库成功后，淘汰缓存出现失败，也会导致数据的不一致。
4. 淘汰缓存成功，但更新数据库时出现失败，导致查询缓存时出现miss情况。

#### 缓存并发问题

如下图示：

![image-20190211111429833](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211111429833.png)

​	当出现缓存不一致情况或某个缓存中数据的key更新后，线程会向数据库请求并查询数据。但发生在多并发情形下，该并发问题会对数据库造成巨大的冲击，甚至会导致缓存雪崩。

如图中所示，为解决缓存并发中的问题，需要对线程访问数据库时加锁：**查询数据库前加锁，在重建缓存后解锁。**

#### 缓存穿透问题

如下图示：![image-20190211111543417](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211111543417.png)

​	在高并发场景下，某一个缓存的key的多线程并发访问中未被命中，由于缓存架构存在访问的容错性，会允许线程从后端获取数据，从而对数据库造成巨大冲击。

当缓存中的key对应的数据为空时，导致大量的无效查询操作，且对数据库造成巨大压力。

避免缓存穿透的解决方案：

1. 对查询结果为空的对象也进行缓存：如果查询的集合数据类型为null，要转换为空的集合；如果缓存时单个对象的null，需要通过字段标识进行区分，避免null的出现。同时需要保持缓存的时效性。实现简单，适用于命中率不高但更新频繁的场景。
2. 单独过滤处理：对所有可能数据为空的key划分统一的区域存放，并在请求前进行拦截。实现较复杂，适用于命中率不高且更新不频繁的场景。

#### 缓存雪崩现象

> 缓存的颠簸（抖动）问题：是一种轻微的缓存雪崩现象，但仍会降低系统性能，并对系统稳定造成巨大的影响。一般是由缓存结点的故障导致。也需要一致性hash算法解决。

由于缓存的问题，导致大量的请求到达后端的数据库，从而导致数据库崩溃，甚至整个系统崩溃。
导致缓存雪崩的情况有：缓存并发问题、缓存穿透、缓存抖动等。如：一个时间点内，缓存中的数据周期性地集中失效，也可能导致雪崩。其中可随机性地设置缓存的失效过期时间，避免集中失效。

## 10.3 消息队列

### 概述

消息队列已经逐步成为企业系统内部通信的核心手段。

简单的消息队列模型：

![image-20190211112104541](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211112104541.png)

​	举例购物网站购物后，需要发送短信通知。其中图中的流程A就是发短信的业务，将其封装为一条消息A1，并放到消息队列，消息队列按照一定处理顺序处理消息，若成功则消息A1被处理；如果因短信调用端接口出现问题，短信发送超时或短时间内达到上限，则导致失败，可以重新将消息放入消息队列。
其中的一些待处理的细节：

1. 在消息中指定最小的执行时间。可减少重新尝试的次数。
2. 需对消息队列对消息的处理速度，即短时间内短信发送的上限。

该例使用消息队列的好处：

1. 实现网站主业务与短信通知业务的异步解耦。
2. 简化设计。短信通知业务交由消息队列处理。
3. 通过消息保持了事务最终一致性。即使失败也会重新尝试，保证短信业务的执行完成。
4. 若还需要邮件通知业务，可以与短信通知业务并行处理；而不必同步等待，，增强系统的异步处理能力，减少甚至消灭并发现象。

### 消息队列使用场景

1. 异步处理：如用户注册后，需要发注册邮件和注册短信。传统的做法有两种 1.串行的方式；2.并行方式
2. 应用解耦：如用户下单后，订单系统需要通知库存系统。传统的做法是，订单系统调用库存系统的接口。
3. 流量销锋：如秒杀活动，一般会因为流量过大，导致流量暴增，应用挂掉。为解决这个问题，一般需要在应用前端加入消息队列。
4. 日志系统：如Kafka的应用，解决大量日志传输的问题。
5. 信息通讯：如实现点对点消息队列，或者聊天室等。
6. 实现最终一致性：可按批次进行消息处理，并保证最终一致性。支持最终一致性的MQ可以用来处理对延迟非敏感的分布式事务场景。相对于较笨重的分布式事务，其不失为较优的处理方式。
7. 广播：消息队列的基础功能。
8. 错峰与流控：通过中间件系统实现。

[文章参考](https://blog.csdn.net/cws1214/article/details/52922267)

> 消息队列并不是万能的：
> 对于需要强事务保证，而且对延迟敏感事务，RPC是由于消息队列MQ的。

### 消息队列特性

1. 业务无关：只做消息分发；
2. FIFO：与缓存区Buffer的本质区别，即先投递先到达；
3. 容灾：结点的动态增删和消息的持久化
4. 性能：吞吐量提升，系统内部通信效率提高。

### 消息队列举例

Kafka
RabbitMQ
ActiveMQ
RocketMQ等

------

#### 初识Kafka

Kafka是Apache基金会下的一个开源项目。
是一个高性能跨语言的分布式发布订阅（PUB/SUB）的消息队列系统。
其处理流程图为:![image-20190211112637201](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211112637201.png)

##### Kafka特性

1. 可快速持久化。通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。
2. 高吞吐量。即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。
3. 完全的分布式系统。它的Broker、Producer、Consumer都原生地支持分布式，自动支持负载均衡。
4. 支持Hadoop的数据并行加载。可通过hadoop的并行加载机制统一处理在线、离线的消息。
5. 支持通过Kafka服务器和消费机集群来分区消息。

##### Kafka相关术语

对应于上图：

- Broker
  Kafka集群包含一个或多个服务器，这种服务器被称为broker。
- Topic
  每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。（物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）
- Partition
  Partition是物理上的概念，每个Topic包含一个或多个Partition.
- Producer
  负责发布消息到Kafka broker
- Consumer
  消息消费者，向Kafka broker读取消息的客户端。
- Consumer Group
  每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）。

#### 初识RabbitMQ

相对于其他消息队列，RabbitMQ有自己的服务器的管理界面。
处理流程图如下：

![image-20190211112701852](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211112701852.png)

##### **其中的概念说明：**

- Broker：简单来说就是消息队列服务器实体。
- Exchange：消息交换机，它指定消息按什么规则，路由到哪个队列。
- Queue：消息队列载体，每个消息都会被投入到一个或多个队列。
- Binding：绑定，它的作用就是把exchange和queue按照路由规则绑定起来。
- Routing Key：路由关键字，exchange根据这个关键字进行消息投递。
- vhost：虚拟主机，一个broker里可以开设多个vhost，用作不同用户的权限分离。
- producer：消息生产者，就是投递消息的程序。
- consumer：消息消费者，就是接受消息的程序。
- channel：消息通道，在客户端的每个连接里，可建立多个channel，每个channel代表一个会话任务。
- ExchangeType（关键）：交换策略。指定exchange分发消息到哪一个或多个队列中。其有四种类型：Direct ， Fanout ， Topic ， Handers。

##### **其处理流程为：**

1. 客户端连接到消息队列服务器，打开一个channel。
2. 客户端声明一个exchange，并设置相关属性。
3. 客户端声明一个queue，并设置相关属性。
4. 客户端使用RoutingKey，在exchange和queue之间建立好绑定关系。
5. 客户端投递消息到exchange。
6. exchange接收到消息后，就根据消息的key和已经设置的binding，进行消息路由，将消息投递到一个或多个队列里。
7. 客户端只需要负责处理消息。

## 10.4 高并发之应用拆分

### 概述

一个服务器再怎么优化，其处理能力都是有限的。之前介绍过过扩容、缓存机制、消息队列等优化方案，都是十分有效的。根据项目情况，将一个整体应用拆分为多个应用也不失为一个方案。比如按功能模块及功能模块使用频率拆分。

例子如下：

![image-20190211113005922](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211113005922.png)

### 应用拆分的好处

1. 减轻并优化了整个统一的应用的压力。
2. 拆分后的应用可以被更精准的监控。
3. 不同子应用会更容易管理及局部优化。
4. 更利于功能模块内部的团队协作。

### 应用拆分的弊端

1. 管理的复杂度上升。
2. 代价昂贵。使用资源的成本增加。
3. 网络开销增加，带宽要求增加。

### 应用拆分的基本原则

1. 业务优先。优先按照业务的功能拆分为小应用。
2. 循序渐进，迭代拆分并进行测试。
3. 兼顾技术：重构、分层。
4. 可靠测试。减少或避免累积错误的出现。

### 应用拆分的思考

1. 应用之间通信：RPC（dubbo等）或消息队列（适用于传输数据包小，但传输量大，对数据的实时性要求不高的场景）。
2. 应用之间的数据库设计：每个应用都应有自己的数据库，其中一些共同的信息可以另建一个公共数据库来存放。
3. 避免事务操作跨应用，降低耦合度。

------

### 服务化的Dubbo

Dubbo也是一种分布式的服务框架，可实现软负载均衡。
但Dubbo Service不是分布式的服务框架，但可以结合其他组件实现负载均衡。
Dubbo还提供了监控中心（可选，需要单独配置）和调用中心。

其处理流程原理图如下：

![image-20190211113028746](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211113028746.png)

其中的Registey模块选择zookeeper；
服务端Provider通常会声明一个Java接口类来代表自己提供的服务。当消费端Consumer获得接口并**配置完相应的内容后**，会**调用相应的接口方法**，底层实际就对应着invoke()方法调用服务，并将结果封装为接口定义的类型返回。

------

### 微服务

微服务是一个架构概念。通过功能分解，对解决方案解耦，并提供更加灵活的服务支持。它可扩展单个组件，而不是整个应用程序堆栈，从而满足服务等级协议。它围绕着业务领域来创建应用，该应用可独立地进行开发、管理、迭代。在分散的组件中，使用云架构和平台式管理、部署和服务功能，使产品交付更加简单。它的本质是通过使用功能明确、业务精炼的服务，去解决更大更实际的问题。

微服务处理流程图如下：

![image-20190211113043896](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211113043896.png)

### 微服务特点

1. 每个微服务独立地组成整个大服务。

2. 单独部署。

3. 分布式地进行管理。

4. 强调隔离性。

   > 实现隔离化的标准：
   > （1）是由分布式服务组成的系统；
   > （2）按照业务划分组织；
   > （3）其为有生命的产品；
   > （4）强服务个体，弱通信；
   > （5）自动化运维；
   > （6）具有高度的容错性；
   > （7）可快速演化迭代；

### 应用微服务需解决的问题

1. **客户端如何访问这些服务？**
   通过客户端与产品服务之间的一个访问代理：API Gateway，提供统一的服务入口，让微服务对前台透明；同时可以聚合后台的产品服务，节省流量，提升性能；提供安全、过滤、流控等API的管理功能。

2. 每个服务之间是如何通信的？

   若采用异步方式，则使用消息队列实现，如Kafka；

   若采用同步方式，则包括：REST和RPC。其中REST可通过SpringBoot或JAX-RS；RPC通常使用Dubbo；

   同步方式与异步方式的区别：

   - 同步调用较为简单，一致性强；但当调用层次深时，易出现调用问题；REST是基于http协议，可跨客户端，更易实现，更加灵活（无语言限制）；RPC优点：传输协议更高效，安全更可控。
   - 异步消息方式在分布式系统中有更加广泛的应用，既能减少调用之间的耦合，又能成为调用之间的缓存，确保消息积压不会冲垮被调用方；同时仍可保证调用方的体验，并继续自己的任务而不至于被后台的性能拖慢。但它的一致性较弱，需要接受数据的最终一致性，并由后台服务实现幂等性。需独立引入一个Broker。

3. **如此多的服务是如何实现的？**
   在微服务架构中，都是有多个拷贝以实现负载均衡。一个服务随时可能下线，也可能因应对访问压力，随时增加新的服务节点。
   通过Zookeeper或其他类似功能实现服务之间的发现功能，做服务注册等信息的分布式管理。当服务上线时，服务启用者将将自己的服务信息注册至Zookeeper，并通过心跳维持长连接，并实时更新连接信息；服务调用者通过Zookeeper来寻找地址，根据可定制的算法，找到一个服务，还可以将服务信息缓存至本地，提高性能。当服务上线后，Zookeeper会通知给服务的客户端。

4. 若一个服务崩溃了该如何解决？
   由于分布式最大的特性就是网络是非可靠的。通过微服务拆分，它可以降低网络不可靠带来的风险，但仍需要一定的安全保障。采取相应的手段降低服务调用链中的连环失效：重试机制、应用的限流、熔断机制、负载均衡、系统降级等。

## 10.5 高并发之初识限流

### 概述

高并发场景下，爆炸性大量的对数据库的请求操作不仅会占用十分高比例的网络带宽，导致其他应用对数据库的请求受阻，还会导致从库与主库的延迟大大增加，降低了从库数据的不准确率，也降低了缓存的命中率。

如下图：![image-20190211113220215](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211113220215.png)

### 限流方式

一般开发高并发系统常见的限流有：限制总并发数、限制瞬时并发数、限制时间窗口内的平均速率；其他还有如限制远程接口调用速率、限制MQ的消费速率。另外还可以根据网络连接数、网络流量、CPU或内存负载等来限流。以减少高并发对系统的影响，最终做到有损服务而不是不服务；限流需要评估好，不可乱用，否则会正常流量出现一些奇怪的问题而导致用户抱怨。 [参考文章](https://blog.csdn.net/g_hongjin/article/details/51649246)

> 其中限制总并发数：如数据库连接池、线程池；
> 限制瞬时并发数：如nginx的limit_conn模块，用来限制瞬时并发连接数；
> 限制时间窗口内的平均速率：如Guava的RateLimiter、nginx的limit_req模块，限制每秒的平均速率；

### 限流算法

1. 计数器法
2. 滑动窗口
3. 漏桶算法
4. 令牌桶算法

#### >计数器法

计数器法是最简单、最易实现的限流算法。通过重复设置计数器，对接口一定时间段内的访问频率进行限制。

**弊端**：存在临界问题。
如下图所示：![image-20190211113246614](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211113246614.png)

如上图所示，在临界的小时间段内，发送了200个请求，导致限流的不成功，可能会导致应用的崩溃。

#### >滑动窗口

滑动窗口可以被看做是一个高精度的计数器算法。其中小窗口的个数越多，对限流中请求的统计会越精确，但占用的系统资源会多。
如下图所示：![image-20190211113258431](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211113258431.png)

其中，虚线包括了6个小窗口，这该6个小窗口组成了一个滑动窗口，滑动窗口对请求数量进行限定；每个小窗口都有一个计数器，都限定了相同的一定时间。每经过该小窗口的时间，滑动窗口就向右侧移动一格，如上图的所示，从而避免了计数器法中的弊端。

#### >漏桶算法

漏桶算法（Leaky Bucket）作为计量工具（The Leaky Bucket Algorithm as a Meter）时，可以用于流量整形（Traffic Shaping）和流量控制（TrafficPolicing）。
其算法示意图如下：

![image-20190211113315460](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211113315460.png)

漏桶算法构建一个容量固定的漏桶，请求数会先放入漏桶，以可控的一定速率流出来，当漏桶满了时，多余的请求会被丢弃。

#### >令牌桶算法

令牌桶算法可以看做是漏桶算法和滑动窗口思想的结合体，构造一个存放固定容量令牌的桶，按照可控的固定速率往桶里添加令牌。
如下图所示：![image-20190211113327432](https://raw.githubusercontent.com/JDawnF/learning_note/master/images/image-20190211113327432.png)

当桶满了时，新添加的令牌会被丢弃或拒绝。当一个请求过来时，该桶就移除一个令牌；当桶中没了令牌时，请求也就无法通过。其中移除令牌是没有延迟时间的，若当设置该延迟时间后，就十分近似漏桶算法了。它通过将桶总量划分为多个令牌的容量，不会造成大量请求的突发，可以很好地解决临界问题。

#### 令牌桶算法与漏桶算法的比较

- 令牌桶是按照固定速率往桶中添加令牌，请求是否被处理需要看桶中令牌是否足够，当令牌数减为零时则拒绝新的请求；
- 漏桶则是按照常量固定速率流出请求，流入请求速率任意，当流入的请求数累积到漏桶容量时，则新流入的请求被拒绝；
- 令牌桶限制的是平均流入速率（允许突发请求，只要有令牌就可以处理，支持一次拿3个令牌，4个令牌），并允许一定程度突发流量；
- 漏桶限制的是常量流出速率（即流出速率是一个固定常量值，比如都是1的速率流出，而不能一次是1，下次又是2），从而平滑突发流入速率；
- 令牌桶允许一定程度的突发，而漏桶主要目的是平滑流入速率；
- 两个算法实现可以一样，但是方向是相反的，对于相同的参数得到的限流效果是一样的。































































































































































































































































参照：https://blog.csdn.net/jesonjoke/column/info/21011

[慕课网](https://www.baidu.com/s?wd=%E6%85%95%E8%AF%BE%E7%BD%91&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)jimin老师的[《Java并发编程与高并发解决方案》](https://www.baidu.com/s?wd=%E3%80%8AJava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%B8%8E%E9%AB%98%E5%B9%B6%E5%8F%91%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E3%80%8B&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)课程

https://suprisemf.github.io/categories/%E9%AB%98%E5%B9%B6%E5%8F%91%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/

